{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parulpandey/Library/CloudStorage/OneDrive-Personal/Ext Github Repos/RAG_UI_fresh/.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up Python path to access project modules\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EMBEDDING_MODEL_PATH = \"sentence-transformers/all-mpnet-base-v2\"  # \n",
    "ASSYMETRIC_EMBEDDING = False  # Flag for asymmetric embedding\n",
    "EMBEDDING_DIMENSION = 768  # Embedding model settings\n",
    "TEXT_CHUNK_SIZE = 300  # Maximum number of characters in each text chunk for\n",
    "OLLAMA_MODEL_NAME = (\"llama3.2:1b\") # Name of the model used in Ollama for chat functionality\n",
    "\n",
    "# Logging\n",
    "LOG_FILE_PATH = \"logs/app.log\"  # File path for the application log file\n",
    "\n",
    "# OpenSearch settings\n",
    "OPENSEARCH_HOST = \"localhost\"  # Hostname for the OpenSearch instance\n",
    "OPENSEARCH_PORT = 9200  # Port number for OpenSearch\n",
    "#OPENSEARCH_INDEX = \"documents\"  # Index name for storing documents in OpenSearch\n",
    "OPENSEARCH_INDEX = \"tech-document-2\" # using plasma etch paper \n",
    "## Use http://localhost:9200\n",
    "# opensearch is runnibg on your local machine instead of a remote server\n",
    "# you have already started the opensearch container using docker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding settings\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for generating embeddings\n",
    "EMBEDDING_DIMENSION = 384  # Embedding dimension for the model\n",
    "ASSYMETRIC_EMBEDDING = False  # Whether to use asymmetric embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "# cleint talks to your host where opensearch is running. Gives tasks like indexing, searching, updating, deleting documents to the host. \n",
    "# An OpenSearch client is the official library (in Python, Java, JS, etc.) that wraps OpenSearch’s REST API, so your app can connect, \n",
    "# query, and manage the cluster more easily.\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_compress=True,\n",
    "    timeout=30,\n",
    "    max_retries=3,\n",
    "    retry_on_timeout=True,\n",
    ")\n",
    "\n",
    "# Check connection\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"Successfully connected to OpenSearch {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to OpenSearch: {e}\")\n",
    "    print(\"Make sure OpenSearch is running on localhost:9200\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Search pipeline 'personal-paper-search-pipeline' exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify pipeline exists\n",
    "from opensearchpy.exceptions import NotFoundError\n",
    "#pipeline_name = \"nlp-search-pipeline\"\n",
    "pipeline_name = \"personal-paper-search-pipeline\"\n",
    "\n",
    "try:\n",
    "    result = client.transport.perform_request(\n",
    "        \"GET\",\n",
    "        f\"/_search/pipeline/{pipeline_name}\"\n",
    "    )\n",
    "    print(f\"\\n✅ Search pipeline '{pipeline_name}' exists.\")\n",
    "    \n",
    "except NotFoundError:\n",
    "    print(f\"\\n⚠️ Search pipeline '{pipeline_name}' does NOT exist.\")\n",
    "    print(\"This is required for hybrid search. Please run the prerequisites notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n🚨 Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does open search combing query text and query embedding results \n",
    "- Since the range of bm25/knn is in different ranges they are normalized first \n",
    "normalized_bm25 = (bm25_score - min_bm25) / (max_bm25 - min_bm25)\n",
    "normalized_knn  = (knn_score - min_knn) / (max_knn - min_knn)\n",
    "final_score = w_text * normalized_bm25 + w_knn * normalized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a hybrid search for the query text using both text-based and vector-based search methods\n",
    "# Text-based search is effective for exact matches (BM25) and keyword relevance, while vector-based search captures semantic meaning (knn) \n",
    "# and context.\n",
    "# takes as input both query text and its embedding vector\n",
    "\n",
    "def hybrid_search(query_text: str, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search combining text-based and vector-based queries.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query for BM25 search\n",
    "        query_embedding (List[float]): The vector embedding for KNN search\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The search results\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"_source\": {\"exclude\": [\"embedding\"]},  # Exclude embeddings from results\n",
    "        \"query\": {\n",
    "            \"hybrid\": {\n",
    "                \"queries\": [\n",
    "                    {\"match\": {\"text\": {\"query\": query_text}}},  # Text-based search\n",
    "                    {\n",
    "                        \"knn\": {\n",
    "                            \"embedding\": {\n",
    "                                \"vector\": query_embedding,\n",
    "                                \"k\": top_k,\n",
    "                            }\n",
    "                        }\n",
    "                    },  # Vector-based search\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nExecuting hybrid search query...\")\n",
    "    try:\n",
    "        # Try with search pipeline parameter (for newer OpenSearch versions)\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body,\n",
    "            params={\"search_pipeline\": \"nlp-search-pipeline\"} # Uses the pipeline for score normalization\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fall back to without pipeline parameter for older versions\n",
    "        print(\"Warning: OpenSearch client doesn't support search_pipeline parameter, using raw query\")\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body\n",
    "        )\n",
    "    \n",
    "    return response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rescore query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'knn': {'embedding': {'vector': [0.1, 0.4, 0.6], 'k': 100}}},\n",
       " 'rescore': {'window_size': 50,\n",
       "  'query': {'rescore_query': {'match': {'text': 'quantum computers'}},\n",
       "   'query_weight': 0.3,\n",
       "   'rescore_query_weight': 0.7}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Combines k-NN search with a text-based rescore query to refine results\n",
    "# The k-NN search retrieves the top 100 nearest neighbors based on the embedding vector\n",
    "# The rescore query then re-evaluates the top 50 results using a text match\n",
    "# The final ranking is a weighted combination of the original k-NN scores and the rescore text match scores\n",
    "\n",
    "## rescore_query_weight tells OpenSearch how much to trust the rescore query relative to the original one.\n",
    "\n",
    "{\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"embedding\": {\n",
    "        \"vector\": [0.1, 0.4, 0.6],\n",
    "        \"k\": 100\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"rescore\": {\n",
    "    \"window_size\": 50,\n",
    "    \"query\": {\n",
    "      \"rescore_query\": { \"match\": { \"text\": \"quantum computers\" } },\n",
    "      \"query_weight\": 0.3,\n",
    "      \"rescore_query_weight\": 0.7\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Process Query and Perform Search\n",
    "\n",
    "Now we'll demonstrate how to:\n",
    "1. Process a search query\n",
    "2. Generate its embedding\n",
    "3. Perform hybrid search to get relevant document chunks\n",
    "\n",
    "Let's try a sample query to test our search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Loads and returns the sentence transformer embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text chunks to embed.\n",
    "        \n",
    "    Returns:\n",
    "        List[numpy.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    model = get_embedding_model()\n",
    "    \n",
    "    # If using asymmetric embeddings, prefix each text with \"passage: \"\n",
    "    if ASSYMETRIC_EMBEDDING:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How can attention mechanism be used for CD predictions?'\n",
      "\n",
      "Generating embedding for query...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Generated embedding with dimension: 384\n",
      "\n",
      "Retrieving top 3 documents...\n",
      "\n",
      "Executing hybrid search query...\n",
      "\n",
      "Search results for query: 'How can attention mechanism be used for CD predictions?'\n",
      "\n",
      "Result 1 (Score: 0.986):\n",
      "Text: model. The acquired OES data are characterized by a gradual change in intensity as the etching process progresses, with different intensity widths at certain critical points where the intensity change...\n",
      "Document: Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\n",
      "\n",
      "Result 2 (Score: 0.358):\n",
      "Text: from the beginning  to the end of the etching process contained  in the entire OES time  series data, and the LSTM model achieved  a high accuracy  of 97.1%. The performance  of  the model was further...\n",
      "Document: Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\n",
      "\n",
      "Result 3 (Score: 0.300):\n",
      "Text: and C, and the peaks related to the reaction products in the reaction surface mechanism, CO, CN, and SiF. The wavelengths used can be found in Table 2. Table 2. Information about the wavelengths used ...\n",
      "Document: Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "#query = \"What is the average rate of ice loss\"\n",
    "#query = \"What are transformers?\"\n",
    "query = \"How can attention mechanism be used for CD predictions?\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "# Generate query embedding\n",
    "print(\"\\nGenerating embedding for query...\")\n",
    "embeddings = generate_embeddings([query])\n",
    "query_embedding = embeddings[0].tolist()\n",
    "print(f\"Generated embedding with dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Set number of results to retrieve\n",
    "top_k = 3\n",
    "print(f\"\\nRetrieving top {top_k} documents...\")\n",
    "\n",
    "# Perform hybrid search\n",
    "results = hybrid_search(query, query_embedding, top_k=top_k)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSearch results for query: '{query}'\\n\")\n",
    "for i, hit in enumerate(results, 1):\n",
    "    print(f\"Result {i} (Score: {hit['_score']:.3f}):\")\n",
    "    print(f\"Text: {hit['_source']['text'][:200]}...\")  # Showing truncated text\n",
    "    print(f\"Document: {hit['_source']['document_name']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Generate Response with Ollama\n",
    "\n",
    "- Added step by step thinking - CHAIN OF THOUGHT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to generate responses with Ollama\n",
    "# def generate_response_with_ollama(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "#     \"\"\"\n",
    "#     Generates a response using Ollama based on search results.\n",
    "    \n",
    "#     Args:\n",
    "#         query (str): The user's question\n",
    "#         results (List[Dict]): The search results from OpenSearch\n",
    "#         model_name (str): The Ollama model to use\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: A tuple containing (prompt, model_name)\n",
    "#     \"\"\"\n",
    "#     # Format context from search results\n",
    "#     context = \"\"\n",
    "#     for i, result in enumerate(results):\n",
    "#         context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "#     # Create prompt template\n",
    "# #    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "# #Think step by step using the provided context before answering. If you cannot find the answer in the context, say so.\n",
    "#     prompt = f\"\"\"you are a precise assistant. Use ONLY  athe provided sources. If the answer is not supported by context, say: Information not found in provided sources. Never invent any information\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {query}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "\n",
    "#     return prompt, model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing prompt template to make it more precise and avoid hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate responses with Ollama\n",
    "def generate_response_with_ollama_2(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Generates a response using Ollama based on search results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        results (List[Dict]): The search results from OpenSearch\n",
    "        model_name (str): The Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing (prompt, model_name)\n",
    "    \"\"\"\n",
    "    # Format context from search results\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you cannot find the answer in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    return prompt, model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring Ollama model llama3.2:1b is available...\n",
      "Model llama3.2:1b is ready.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "model. The acquired OES data are characterized by a gradual change in intensity as the etching process progresses, with different intensity widths at certain critical points where the intensity changes in unit time. To effectively capture these change features, we used an attention mechanism. The attention mechanism can focus on points in a sequence of OES data that have different intensity changes. It emphasizes the important parts of the sequence by assigning higher weights to points where the change in intensity represents a change from the previous state. During the learning process of the model, it evaluates the importance of each element of the input sequence and helps to ensure that important information is not lost [ 30]. The attention-based LSTM model was designed as shown in Figure 7. It consists of a basic LSTM layer that processes the input data, followed by an attention layer that evaluates the importance of each sequence. Finally, a fully connected layer is added to receive the output from the attention layer and generate the final predicted values. This ultimately produces an output that identifies the situations before and after the etch endpoint. The structure and parameter values of the attention-based LSTM model can be found in Table 7. Table 7. Attention-based LSTM model structure and parameter values. Layer (Type) Output Shape Parameter No. lstm (LSTM) (None, 10, 64) 18,688 lstm_1 (LSTM) (None, 10, 32) 12,416 Attention (Attention) (None, 32) 1088 dense (Dense) (None, 16) 528 dense_1 (Dense) (None, 1) 17 The training process for the attention-based LSTM model is the same as that of the LSTM model, with the only difference being the addition of the attention layer. When the training of the attention-based LSTM model was completed, it achieved an accuracy of approximately 98.2%. The performance of the model was evaluated using 10-fold crossvalidation and the loss and accuracy graphs of the model. The results showed that, similarElectronics 2024 ,13, 3577 9 of 11 to the LSTM model, the validation loss exhibited a similar trend, and no signs of overfitting were observed. Electronics  2024, 13, x FOR PEER REVIEW   9 of 11     to the LSTM model, the validation  loss exhibited  a similar trend, and no signs of overﬁtting were observed.     Figure 7. Attention-based  LSTM neural network schematic.   As shown in Table 8, the attention-based  LSTM model improved  the etch endpoint   detection  performance  by approximately  1% and reduced the false detection  rate (F1  score) by approximately  1% compared  to the LSTM model.  Table 8. Attention-based  LSTM model confusion  matrix metrics.    Precision   Recall  F1 Score  Without EPD  0.98  0.98  0.98  With EPD  0.98  0.98  0.98  Accuracy     0.98  4. Conclusions   In this study, we proposed  an LSTM-based  method for detecting  the endpoint  in  etching processes.  Unlike traditional  approaches  that rely on signal patterns at speciﬁc  time points, this method demonstrated  its eﬀectiveness  by considering  all the information   from the beginning  to the end of the etching process contained  in the entire OES time  series data, and the LSTM model achieved  a high accuracy  of 97.1%. The performance  of  the model was further improved  by applying  the attention mechanism  to the LSTM  model. The attention mechanism  assigns higher weights to points in the time series data  where the intensity  change diﬀers from the previous  state, allowing  the model to focus on  the critical information  for etch endpoint  detection.  This resulted in an approximately  1%  performance  improvement  compared  to the LSTM model, with the attention-based  LSTM  model ultimately  achieving  a high accuracy  of 98.2%. The proposed  attention-based  LSTM  model overcomes  the limitations  of the existing EPD algorithms  and addresses  the issues  of traditional  methods  that primarily  focus on signal changes only at the precise EPD moment. By utilizing the temporal  dependencies  inherent in the entire OES time series data,  the proposed  model not only accurately  identi ﬁes the EPD moment but also captures  the  precursory  symptoms  of EPD occurrence,  enhancing  the reliability  of the system. In addition to improving  endpoint  detection  accuracy,  the proposed  method also reduced the  false detection  rate, which can negatively  impact yield. This improvement  can contribute   to enhanced  EPD accuracy  in high-aspect-ratio  etching processes.  Furthermore,  the proposed LSTM-based  method can be applied not only to the etching process but also to various stages of the semiconductor  manufacturing  process, such as cleaning processes,  process monitoring,  and anomaly  detection.  This can be utilized in various ﬁelds where time  Figure 7. Attention-based LSTM neural network schematic. As shown in Table 8, the attention-based LSTM model improved the etch endpoint detection performance\n",
      "\n",
      "Document 2:\n",
      "from the beginning  to the end of the etching process contained  in the entire OES time  series data, and the LSTM model achieved  a high accuracy  of 97.1%. The performance  of  the model was further improved  by applying  the attention mechanism  to the LSTM  model. The attention mechanism  assigns higher weights to points in the time series data  where the intensity  change diﬀers from the previous  state, allowing  the model to focus on  the critical information  for etch endpoint  detection.  This resulted in an approximately  1%  performance  improvement  compared  to the LSTM model, with the attention-based  LSTM  model ultimately  achieving  a high accuracy  of 98.2%. The proposed  attention-based  LSTM  model overcomes  the limitations  of the existing EPD algorithms  and addresses  the issues  of traditional  methods  that primarily  focus on signal changes only at the precise EPD moment. By utilizing the temporal  dependencies  inherent in the entire OES time series data,  the proposed  model not only accurately  identi ﬁes the EPD moment but also captures  the  precursory  symptoms  of EPD occurrence,  enhancing  the reliability  of the system. In addition to improving  endpoint  detection  accuracy,  the proposed  method also reduced the  false detection  rate, which can negatively  impact yield. This improvement  can contribute   to enhanced  EPD accuracy  in high-aspect-ratio  etching processes.  Furthermore,  the proposed LSTM-based  method can be applied not only to the etching process but also to various stages of the semiconductor  manufacturing  process, such as cleaning processes,  process monitoring,  and anomaly  detection.  This can be utilized in various ﬁelds where time  Figure 7. Attention-based LSTM neural network schematic. As shown in Table 8, the attention-based LSTM model improved the etch endpoint detection performance by approximately 1% and reduced the false detection rate (F1 score) by approximately 1% compared to the LSTM model. Table 8. Attention-based LSTM model confusion matrix metrics. Precision Recall F1 Score Without EPD 0.98 0.98 0.98 With EPD 0.98 0.98 0.98 Accuracy 0.98 4. Conclusions In this study, we proposed an LSTM-based method for detecting the endpoint in etching processes. Unlike traditional approaches that rely on signal patterns at specific time points, this method demonstrated its effectiveness by considering all the information from the beginning to the end of the etching process contained in the entire OES time series data, and the LSTM model achieved a high accuracy of 97.1%. The performance of the model was further improved by applying the attention mechanism to the LSTM model. The attention mechanism assigns higher weights to points in the time series data where the intensity change differs from the previous state, allowing the model to focus on the critical information for etch endpoint detection. This resulted in an approximately 1% performance improvement compared to the LSTM model, with the attention-based LSTM model ultimately achieving a high accuracy of 98.2%. The proposed attention-based LSTM model overcomes the limitations of the existing EPD algorithms and addresses the issues of traditional methods that primarily focus on signal changes only at the precise EPD moment. By utilizing the temporal dependencies inherent in the entire OES time series data, the proposed model not only accurately identifies the EPD moment but also captures the precursory symptoms of EPD occurrence, enhancing the reliability of the system. In addition to improving endpoint detection accuracy, the proposed method also reduced the false detection rate, which can negatively impact yield. This improvement can contribute to enhanced EPD accuracy in high-aspect-ratio etching processes. Furthermore, the proposed LSTM-based method can be applied not only to the etching process but also to various stages of the semiconductor manufacturing process, such as cleaning processes, process monitoring, and anomaly detection. This can be utilized in various fields where timeElectronics 2024 ,13, 3577 10 of 11 series data analysis plays a crucial role and is expected to contribute to the technological advancement of semiconductor manufacturing processes. Author Contributions: Conceptualization, S.J.H., Y.J.K. and J.H.S. (Jong Hyeon Shin); methodology, S.J.H., Y.J.K. and K.H.C.; software, Y.J.K. and J.H.S. (Jong Hyeon Shin); validation, J.S.K. and J.H.S. (Jung Ho Song); formal analysis, Y.J.K. and S.J.H.; investigation, Y.J.K., J.H.S. (Jong Hyeon Shin) and S.J.H.;\n",
      "\n",
      "Document 3:\n",
      "and C, and the peaks related to the reaction products in the reaction surface mechanism, CO, CN, and SiF. The wavelengths used can be found in Table 2. Table 2. Information about the wavelengths used for EPD. Species Wavelength (nm) SiF 336, 440 CO 482, 560, 561 CN 386, 387 F 685, 677, 703 C2 516 O 777, 844Electronics 2024 ,13, 3577 4 of 11 All coupon wafers were etched for a pre-determined fixed time. To ensure the complete etching of the SiO 2layer, over-etching was performed. During this process, it was confirmed through ellipsometry that approximately 200 nm of the Si 3N4layer was etched. This was carried out to verify that the SiO 2layer was fully etched. Therefore, the optical emission signals collected through OES serve as time series data spanning the entire etching process. By using these data as input for the LSTM model, the model can learn long-term dependencies and detect the etch endpoint. 3. LSTM Method The LSTM is a model designed to address the limitations of the RNN, specifically the problems of vanishing gradients and exploding gradients. The gradient information in the LSTM is more effective in maintaining and utilizing information over longer sequences compared to the RNN [27]. As shown in Figure 2, the LSTM consists of three gates: the input gate, forget gate, and output gate. Each gate has the following roles. The input gate determines how to update the internal state based on the current state and the previous hidden state [ 28]. It decides what information should be stored in the cell state. First, it determines what information to store from the current input and the previous hidden state through a sigmoid layer. Then, the hyperbolic tangent (tanh) layer generates a new candidate vector. The forget gate determines what information should be forgotten and to what extent, from the previous cell state, by passing it through a sigmoid layer. The output gate determines how much of the cell state should be reflected in the hidden state and decides the hidden state to be passed to the next time step of the LSTM [28]. Electronics  2024, 13, x FOR PEER REVIEW   4 of 11     All coupon wafers were etched for a pre-determined  ﬁxed time. To ensure the complete etching of the SiO 2 layer, over-etching  was performed.  During this process, it was  conﬁrmed through ellipsometry  that approximately  200 nm of the Si3N4 layer was etched.  This was carried out to verify that the SiO 2 layer was fully etched. Therefore,  the optical  emission  signals collected  through OES serve as time series data spanning  the entire etching process. By using these data as input for the LSTM model, the model can learn longterm dependencies  and detect the etch endpoint.   3. LSTM Method  The LSTM is a model designed  to address the limitations  of the RNN, speciﬁcally the  problems  of vanishing  gradients  and exploding  gradients.  The gradient information  in the  LSTM is more eﬀective in maintaining  and utilizing information  over longer sequences   compared  to the RNN [27].  As shown in Figure 2, the LSTM consists of three gates: the input gate, forget gate,  and output gate. Each gate has the following  roles. The input gate determines  how to update the internal state based on the current state and the previous  hidden state [28]. It  decides what information  should be stored in the cell state. First, it determines  what information  to store from the current input and the previous  hidden state through a sigmoid  layer. Then, the hyperbolic  tangent (tanh) layer generates  a new candidate  vector. The  forget gate determines  what information  should be forgotten and to what extent, from the  previous  cell state, by passing it through a sigmoid layer. The output gate determines  how  much of the cell state should be reﬂected in the hidden state and decides the hidden state  to be passed to the next time step of the LSTM [28].    Figure 2. LSTM diagram [29].  The LSTM controls the cell state using three gates. The cell state contains all the core  information,  and the hidden state is processed  whenever  necessary  to propagate  information in a form that exposes only the required  information  for each time step. The equations for the LSTM are as follows:  𝑓௧ൌ 𝜎ሺ𝑊௙∙ሾℎ௧ିଵ ,𝑥௧ሿ൅𝑏௙ሻ,  (1) 𝑖௧ൌ 𝜎൫𝑊௜∙ൣℎ௧ିଵ ,𝑥௧൧൅𝑏௜൯,   (2) 𝐶⃗௧ൌ tanhሺ𝑊௖∙ሾℎ௧ିଵ ,𝑥௧ሿ൅𝑏௖ሻ,   (3) 𝐶௧ൌ 𝑓௧∙𝐶௧ିଵ൅𝑖௧∙ 𝐶⃗ ௧,  (4) 𝑜௧ൌ 𝜎ሺ𝑊௢∙ሾℎ௧ିଵ ,𝑥௧ሿ൅𝑏௢ሻ,  (5) ℎ௧ൌ 𝑜௜∙tanh ሺ𝐶௧ሻ  (6) Figure 2. LSTM diagram [29]. The LSTM controls the cell state using three gates. The cell state contains all the core information, and the hidden state is processed whenever necessary to propagate information in a form that exposes only the required information for each time step. The equations for the LSTM\n",
      "\n",
      "\n",
      "\n",
      "Question: How can attention mechanism be used for CD predictions?\n",
      "\n",
      "Answer: \n",
      "\n",
      "Using model: llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "# Ensure model is pulled\n",
    "print(f\"Ensuring Ollama model {OLLAMA_MODEL_NAME} is available...\")\n",
    "try:\n",
    "    ollama.pull(OLLAMA_MODEL_NAME)\n",
    "    print(f\"Model {OLLAMA_MODEL_NAME} is ready.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Error pulling model: {e.error}\")\n",
    "    print(\"You might need to install the model manually with: ollama pull \" + OLLAMA_MODEL_NAME)\n",
    "\n",
    "# Get prompt and model\n",
    "prompt, model_name = generate_response_with_ollama_2(query, results)\n",
    "print('\\n\\n\\n')\n",
    "print(prompt)\n",
    "print(f\"\\nUsing model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt created with 14490 characters\n",
      "First 200 characters of prompt:\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "model. The acquired OES data are character...\n"
     ]
    }
   ],
   "source": [
    "# Print prompt length\n",
    "print(f\"\\nPrompt created with {len(prompt)} characters\")\n",
    "print(\"First 200 characters of prompt:\")\n",
    "print(prompt[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response with Ollama...\n",
      "\n",
      "Response:\n",
      "Information not found.\n",
      "\n",
      "Response generation complete!\n",
      "Generated 22 characters\n"
     ]
    }
   ],
   "source": [
    "# Give promt as input to ollama generate function\n",
    "\n",
    "# Generate streaming response in Ollama: Setting stream=True when calling ollama.generate(...) \n",
    "# tells the Ollama client to return an iterator/stream of partial response chunks instead of waiting for the full answer. \n",
    "\n",
    "print(\"\\nGenerating response with Ollama...\")\n",
    "response = \"\"\n",
    "print(\"\\nResponse:\")\n",
    "for chunk in ollama.generate(model=model_name, prompt=prompt, stream=True, \n",
    "        system=\"You are a retrieval-grounded assistant. Use ONLY the provided context. \"\n",
    "           \"If the answer is not found, reply exactly: information not found.\",\n",
    "        options={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.0,\n",
    "        \"repeat_penalty\": 1.2,\n",
    "        \"num_ctx\": 8192\n",
    "    }):\n",
    "    piece = chunk['response']\n",
    "    print(piece, end='', flush=True)\n",
    "    response += piece\n",
    "\n",
    "print(\"\\n\\nResponse generation complete!\")\n",
    "print(f\"Generated {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other things that can be tried in this project\n",
    "- Use different embedding models (e.g., OpenAI, HuggingFace)\n",
    "\n",
    "\n",
    "## LLM prompting improvements\n",
    " - Instruction tuning / style\n",
    "“Answer concisely using the context.” vs. “Provide a detailed technical explanation citing sources.”\n",
    "\n",
    "- Chain-of-thought prompting\n",
    "Add reasoning steps explicitly:\n",
    "Think step by step using the provided context before answering.\n",
    "\n",
    "\n",
    "- Citation prompting\n",
    "Tell model to reference [source id] tokens.\n",
    "\n",
    "- Guardrails / hallucination control\n",
    "\n",
    "- Add explicit constraint:\n",
    "If the answer is not in the context, reply \"Information not found\".\n",
    "\n",
    "\n",
    "- Persona or role\n",
    "“You are a semiconductor process expert explaining to an engineer.”\n",
    "\n",
    "- Answer formatting\n",
    "Structured JSON or markdown summaries for downstream use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
