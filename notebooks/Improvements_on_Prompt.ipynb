{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parulpandey/Library/CloudStorage/OneDrive-Personal/Ext Github Repos/RAG_UI_fresh/.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up Python path to access project modules\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EMBEDDING_MODEL_PATH = \"sentence-transformers/all-mpnet-base-v2\"  # \n",
    "ASSYMETRIC_EMBEDDING = False  # Flag for asymmetric embedding\n",
    "EMBEDDING_DIMENSION = 768  # Embedding model settings\n",
    "TEXT_CHUNK_SIZE = 300  # Maximum number of characters in each text chunk for\n",
    "OLLAMA_MODEL_NAME = (\"llama3.2:1b\") # Name of the model used in Ollama for chat functionality\n",
    "\n",
    "# Logging\n",
    "LOG_FILE_PATH = \"logs/app.log\"  # File path for the application log file\n",
    "\n",
    "# OpenSearch settings\n",
    "OPENSEARCH_HOST = \"localhost\"  # Hostname for the OpenSearch instance\n",
    "OPENSEARCH_PORT = 9200  # Port number for OpenSearch\n",
    "#OPENSEARCH_INDEX = \"documents\"  # Index name for storing documents in OpenSearch\n",
    "OPENSEARCH_INDEX = \"tech-document-2\" # using plasma etch paper \n",
    "## Use http://localhost:9200\n",
    "# opensearch is runnibg on your local machine instead of a remote server\n",
    "# you have already started the opensearch container using docker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding settings\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for generating embeddings\n",
    "EMBEDDING_DIMENSION = 384  # Embedding dimension for the model\n",
    "ASSYMETRIC_EMBEDDING = False  # Whether to use asymmetric embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "# cleint talks to your host where opensearch is running. Gives tasks like indexing, searching, updating, deleting documents to the host. \n",
    "# An OpenSearch client is the official library (in Python, Java, JS, etc.) that wraps OpenSearchâ€™s REST API, so your app can connect, \n",
    "# query, and manage the cluster more easily.\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_compress=True,\n",
    "    timeout=30,\n",
    "    max_retries=3,\n",
    "    retry_on_timeout=True,\n",
    ")\n",
    "\n",
    "# Check connection\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"Successfully connected to OpenSearch {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to OpenSearch: {e}\")\n",
    "    print(\"Make sure OpenSearch is running on localhost:9200\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Search pipeline 'personal-paper-search-pipeline' exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify pipeline exists\n",
    "from opensearchpy.exceptions import NotFoundError\n",
    "#pipeline_name = \"nlp-search-pipeline\"\n",
    "pipeline_name = \"personal-paper-search-pipeline\"\n",
    "\n",
    "try:\n",
    "    result = client.transport.perform_request(\n",
    "        \"GET\",\n",
    "        f\"/_search/pipeline/{pipeline_name}\"\n",
    "    )\n",
    "    print(f\"\\nâœ… Search pipeline '{pipeline_name}' exists.\")\n",
    "    \n",
    "except NotFoundError:\n",
    "    print(f\"\\nâš ï¸ Search pipeline '{pipeline_name}' does NOT exist.\")\n",
    "    print(\"This is required for hybrid search. Please run the prerequisites notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nğŸš¨ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does open search combing query text and query embedding results \n",
    "- Since the range of bm25/knn is in different ranges they are normalized first \n",
    "normalized_bm25 = (bm25_score - min_bm25) / (max_bm25 - min_bm25)\n",
    "normalized_knn  = (knn_score - min_knn) / (max_knn - min_knn)\n",
    "final_score = w_text * normalized_bm25 + w_knn * normalized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a hybrid search for the query text using both text-based and vector-based search methods\n",
    "# Text-based search is effective for exact matches (BM25) and keyword relevance, while vector-based search captures semantic meaning (knn) \n",
    "# and context.\n",
    "# takes as input both query text and its embedding vector\n",
    "\n",
    "def hybrid_search(query_text: str, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search combining text-based and vector-based queries.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query for BM25 search\n",
    "        query_embedding (List[float]): The vector embedding for KNN search\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The search results\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"_source\": {\"exclude\": [\"embedding\"]},  # Exclude embeddings from results\n",
    "        \"query\": {\n",
    "            \"hybrid\": {\n",
    "                \"queries\": [\n",
    "                    {\"match\": {\"text\": {\"query\": query_text}}},  # Text-based search\n",
    "                    {\n",
    "                        \"knn\": {\n",
    "                            \"embedding\": {\n",
    "                                \"vector\": query_embedding,\n",
    "                                \"k\": top_k,\n",
    "                            }\n",
    "                        }\n",
    "                    },  # Vector-based search\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nExecuting hybrid search query...\")\n",
    "    try:\n",
    "        # Try with search pipeline parameter (for newer OpenSearch versions)\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body,\n",
    "            params={\"search_pipeline\": \"nlp-search-pipeline\"} # Uses the pipeline for score normalization\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fall back to without pipeline parameter for older versions\n",
    "        print(\"Warning: OpenSearch client doesn't support search_pipeline parameter, using raw query\")\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body\n",
    "        )\n",
    "    \n",
    "    return response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rescore query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'knn': {'embedding': {'vector': [0.1, 0.4, 0.6], 'k': 100}}},\n",
       " 'rescore': {'window_size': 50,\n",
       "  'query': {'rescore_query': {'match': {'text': 'quantum computers'}},\n",
       "   'query_weight': 0.3,\n",
       "   'rescore_query_weight': 0.7}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Combines k-NN search with a text-based rescore query to refine results\n",
    "# The k-NN search retrieves the top 100 nearest neighbors based on the embedding vector\n",
    "# The rescore query then re-evaluates the top 50 results using a text match\n",
    "# The final ranking is a weighted combination of the original k-NN scores and the rescore text match scores\n",
    "\n",
    "## rescore_query_weight tells OpenSearch how much to trust the rescore query relative to the original one.\n",
    "\n",
    "{\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"embedding\": {\n",
    "        \"vector\": [0.1, 0.4, 0.6],\n",
    "        \"k\": 100\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"rescore\": {\n",
    "    \"window_size\": 50,\n",
    "    \"query\": {\n",
    "      \"rescore_query\": { \"match\": { \"text\": \"quantum computers\" } },\n",
    "      \"query_weight\": 0.3,\n",
    "      \"rescore_query_weight\": 0.7\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Process Query and Perform Search\n",
    "\n",
    "Now we'll demonstrate how to:\n",
    "1. Process a search query\n",
    "2. Generate its embedding\n",
    "3. Perform hybrid search to get relevant document chunks\n",
    "\n",
    "Let's try a sample query to test our search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Loads and returns the sentence transformer embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text chunks to embed.\n",
    "        \n",
    "    Returns:\n",
    "        List[numpy.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    model = get_embedding_model()\n",
    "    \n",
    "    # If using asymmetric embeddings, prefix each text with \"passage: \"\n",
    "    if ASSYMETRIC_EMBEDDING:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How can attention mechanism be used for CD predictions?'\n",
      "\n",
      "Generating embedding for query...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Generated embedding with dimension: 384\n",
      "\n",
      "Retrieving top 3 documents...\n",
      "\n",
      "Executing hybrid search query...\n",
      "\n",
      "Search results for query: 'How can attention mechanism be used for CD predictions?'\n",
      "\n",
      "Result 1 (Score: 0.986):\n",
      "Text: model. The acquired OES data are characterized by a gradual change in intensity as the etching process progresses, with different intensity widths at certain critical points where the intensity change...\n",
      "Document: Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\n",
      "\n",
      "Result 2 (Score: 0.358):\n",
      "Text: fromÂ theÂ beginning Â toÂ theÂ endÂ ofÂ theÂ etchingÂ processÂ contained Â inÂ theÂ entireÂ OESÂ timeÂ  seriesÂ data,Â andÂ theÂ LSTMÂ modelÂ achieved Â aÂ highÂ accuracy Â ofÂ 97.1%.Â TheÂ performance Â ofÂ  theÂ modelÂ wasÂ further...\n",
      "Document: Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\n",
      "\n",
      "Result 3 (Score: 0.300):\n",
      "Text: and C, and the peaks related to the reaction products in the reaction surface mechanism, CO, CN, and SiF. The wavelengths used can be found in Table 2. Table 2. Information about the wavelengths used ...\n",
      "Document: Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "#query = \"What is the average rate of ice loss\"\n",
    "#query = \"What are transformers?\"\n",
    "query = \"How can attention mechanism be used for CD predictions?\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "# Generate query embedding\n",
    "print(\"\\nGenerating embedding for query...\")\n",
    "embeddings = generate_embeddings([query])\n",
    "query_embedding = embeddings[0].tolist()\n",
    "print(f\"Generated embedding with dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Set number of results to retrieve\n",
    "top_k = 3\n",
    "print(f\"\\nRetrieving top {top_k} documents...\")\n",
    "\n",
    "# Perform hybrid search\n",
    "results = hybrid_search(query, query_embedding, top_k=top_k)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSearch results for query: '{query}'\\n\")\n",
    "for i, hit in enumerate(results, 1):\n",
    "    print(f\"Result {i} (Score: {hit['_score']:.3f}):\")\n",
    "    print(f\"Text: {hit['_source']['text'][:200]}...\")  # Showing truncated text\n",
    "    print(f\"Document: {hit['_source']['document_name']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Generate Response with Ollama\n",
    "\n",
    "- Added step by step thinking - CHAIN OF THOUGHT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to generate responses with Ollama\n",
    "# def generate_response_with_ollama(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "#     \"\"\"\n",
    "#     Generates a response using Ollama based on search results.\n",
    "    \n",
    "#     Args:\n",
    "#         query (str): The user's question\n",
    "#         results (List[Dict]): The search results from OpenSearch\n",
    "#         model_name (str): The Ollama model to use\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: A tuple containing (prompt, model_name)\n",
    "#     \"\"\"\n",
    "#     # Format context from search results\n",
    "#     context = \"\"\n",
    "#     for i, result in enumerate(results):\n",
    "#         context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "#     # Create prompt template\n",
    "# #    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "# #Think step by step using the provided context before answering. If you cannot find the answer in the context, say so.\n",
    "#     prompt = f\"\"\"you are a precise assistant. Use ONLY  athe provided sources. If the answer is not supported by context, say: Information not found in provided sources. Never invent any information\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {query}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "\n",
    "#     return prompt, model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing prompt template to make it more precise and avoid hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate responses with Ollama\n",
    "def generate_response_with_ollama_2(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Generates a response using Ollama based on search results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        results (List[Dict]): The search results from OpenSearch\n",
    "        model_name (str): The Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing (prompt, model_name)\n",
    "    \"\"\"\n",
    "    # Format context from search results\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you cannot find the answer in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    return prompt, model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring Ollama model llama3.2:1b is available...\n",
      "Model llama3.2:1b is ready.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "model. The acquired OES data are characterized by a gradual change in intensity as the etching process progresses, with different intensity widths at certain critical points where the intensity changes in unit time. To effectively capture these change features, we used an attention mechanism. The attention mechanism can focus on points in a sequence of OES data that have different intensity changes. It emphasizes the important parts of the sequence by assigning higher weights to points where the change in intensity represents a change from the previous state. During the learning process of the model, it evaluates the importance of each element of the input sequence and helps to ensure that important information is not lost [ 30]. The attention-based LSTM model was designed as shown in Figure 7. It consists of a basic LSTM layer that processes the input data, followed by an attention layer that evaluates the importance of each sequence. Finally, a fully connected layer is added to receive the output from the attention layer and generate the final predicted values. This ultimately produces an output that identifies the situations before and after the etch endpoint. The structure and parameter values of the attention-based LSTM model can be found in Table 7. Table 7. Attention-based LSTM model structure and parameter values. Layer (Type) Output Shape Parameter No. lstm (LSTM) (None, 10, 64) 18,688 lstm_1 (LSTM) (None, 10, 32) 12,416 Attention (Attention) (None, 32) 1088 dense (Dense) (None, 16) 528 dense_1 (Dense) (None, 1) 17 The training process for the attention-based LSTM model is the same as that of the LSTM model, with the only difference being the addition of the attention layer. When the training of the attention-based LSTM model was completed, it achieved an accuracy of approximately 98.2%. The performance of the model was evaluated using 10-fold crossvalidation and the loss and accuracy graphs of the model. The results showed that, similarElectronics 2024 ,13, 3577 9 of 11 to the LSTM model, the validation loss exhibited a similar trend, and no signs of overfitting were observed. Electronics Â 2024,Â 13,Â xÂ FORÂ PEERÂ REVIEW Â  9Â ofÂ 11Â  Â  Â toÂ theÂ LSTMÂ model,Â theÂ validation Â lossÂ exhibited Â aÂ similarÂ trend,Â andÂ noÂ signsÂ ofÂ overï¬ttingÂ wereÂ observed. Â  Â  FigureÂ 7.Â Attention-based Â LSTMÂ neuralÂ networkÂ schematic. Â  AsÂ shownÂ inÂ TableÂ 8,Â theÂ attention-based Â LSTMÂ modelÂ improved Â theÂ etchÂ endpoint Â  detection Â performance Â byÂ approximately Â 1%Â andÂ reducedÂ theÂ falseÂ detection Â rateÂ (F1Â  score)Â byÂ approximately Â 1%Â compared Â toÂ theÂ LSTMÂ model.Â  TableÂ 8.Â Attention-based Â LSTMÂ modelÂ confusion Â matrixÂ metrics.Â  Â  Precision Â  RecallÂ  F1Â ScoreÂ  WithoutÂ EPDÂ  0.98Â  0.98Â  0.98Â  WithÂ EPDÂ  0.98Â  0.98Â  0.98Â  Accuracy Â Â Â  0.98Â  4.Â Conclusions Â  InÂ thisÂ study,Â weÂ proposed Â anÂ LSTM-based Â methodÂ forÂ detecting Â theÂ endpoint Â inÂ  etchingÂ processes. Â UnlikeÂ traditional Â approaches Â thatÂ relyÂ onÂ signalÂ patternsÂ atÂ speciï¬cÂ  timeÂ points,Â thisÂ methodÂ demonstrated Â itsÂ eï¬€ectiveness Â byÂ considering Â allÂ theÂ information Â  fromÂ theÂ beginning Â toÂ theÂ endÂ ofÂ theÂ etchingÂ processÂ contained Â inÂ theÂ entireÂ OESÂ timeÂ  seriesÂ data,Â andÂ theÂ LSTMÂ modelÂ achieved Â aÂ highÂ accuracy Â ofÂ 97.1%.Â TheÂ performance Â ofÂ  theÂ modelÂ wasÂ furtherÂ improved Â byÂ applying Â theÂ attentionÂ mechanism Â toÂ theÂ LSTMÂ  model.Â TheÂ attentionÂ mechanism Â assignsÂ higherÂ weightsÂ toÂ pointsÂ inÂ theÂ timeÂ seriesÂ dataÂ  whereÂ theÂ intensity Â changeÂ diï¬€ersÂ fromÂ theÂ previous Â state,Â allowing Â theÂ modelÂ toÂ focusÂ onÂ  theÂ criticalÂ information Â forÂ etchÂ endpoint Â detection. Â ThisÂ resultedÂ inÂ anÂ approximately Â 1%Â  performance Â improvement Â compared Â toÂ theÂ LSTMÂ model,Â withÂ theÂ attention-based Â LSTMÂ  modelÂ ultimately Â achieving Â aÂ highÂ accuracy Â ofÂ 98.2%.Â TheÂ proposed Â attention-based Â LSTMÂ  modelÂ overcomes Â theÂ limitations Â ofÂ theÂ existingÂ EPDÂ algorithms Â andÂ addresses Â theÂ issuesÂ  ofÂ traditional Â methods Â thatÂ primarily Â focusÂ onÂ signalÂ changesÂ onlyÂ atÂ theÂ preciseÂ EPDÂ moment.Â ByÂ utilizingÂ theÂ temporal Â dependencies Â inherentÂ inÂ theÂ entireÂ OESÂ timeÂ seriesÂ data,Â  theÂ proposed Â modelÂ notÂ onlyÂ accurately Â identi ï¬esÂ theÂ EPDÂ momentÂ butÂ alsoÂ captures Â theÂ  precursory Â symptoms Â ofÂ EPDÂ occurrence, Â enhancing Â theÂ reliability Â ofÂ theÂ system.Â InÂ additionÂ toÂ improving Â endpoint Â detection Â accuracy, Â theÂ proposed Â methodÂ alsoÂ reducedÂ theÂ  falseÂ detection Â rate,Â whichÂ canÂ negatively Â impactÂ yield.Â ThisÂ improvement Â canÂ contribute Â  toÂ enhanced Â EPDÂ accuracy Â inÂ high-aspect-ratio Â etchingÂ processes. Â Furthermore, Â theÂ proposedÂ LSTM-based Â methodÂ canÂ beÂ appliedÂ notÂ onlyÂ toÂ theÂ etchingÂ processÂ butÂ alsoÂ toÂ variousÂ stagesÂ ofÂ theÂ semiconductor Â manufacturing Â process,Â suchÂ asÂ cleaningÂ processes, Â processÂ monitoring, Â andÂ anomaly Â detection. Â ThisÂ canÂ beÂ utilizedÂ inÂ variousÂ ï¬eldsÂ whereÂ timeÂ  Figure 7. Attention-based LSTM neural network schematic. As shown in Table 8, the attention-based LSTM model improved the etch endpoint detection performance\n",
      "\n",
      "Document 2:\n",
      "fromÂ theÂ beginning Â toÂ theÂ endÂ ofÂ theÂ etchingÂ processÂ contained Â inÂ theÂ entireÂ OESÂ timeÂ  seriesÂ data,Â andÂ theÂ LSTMÂ modelÂ achieved Â aÂ highÂ accuracy Â ofÂ 97.1%.Â TheÂ performance Â ofÂ  theÂ modelÂ wasÂ furtherÂ improved Â byÂ applying Â theÂ attentionÂ mechanism Â toÂ theÂ LSTMÂ  model.Â TheÂ attentionÂ mechanism Â assignsÂ higherÂ weightsÂ toÂ pointsÂ inÂ theÂ timeÂ seriesÂ dataÂ  whereÂ theÂ intensity Â changeÂ diï¬€ersÂ fromÂ theÂ previous Â state,Â allowing Â theÂ modelÂ toÂ focusÂ onÂ  theÂ criticalÂ information Â forÂ etchÂ endpoint Â detection. Â ThisÂ resultedÂ inÂ anÂ approximately Â 1%Â  performance Â improvement Â compared Â toÂ theÂ LSTMÂ model,Â withÂ theÂ attention-based Â LSTMÂ  modelÂ ultimately Â achieving Â aÂ highÂ accuracy Â ofÂ 98.2%.Â TheÂ proposed Â attention-based Â LSTMÂ  modelÂ overcomes Â theÂ limitations Â ofÂ theÂ existingÂ EPDÂ algorithms Â andÂ addresses Â theÂ issuesÂ  ofÂ traditional Â methods Â thatÂ primarily Â focusÂ onÂ signalÂ changesÂ onlyÂ atÂ theÂ preciseÂ EPDÂ moment.Â ByÂ utilizingÂ theÂ temporal Â dependencies Â inherentÂ inÂ theÂ entireÂ OESÂ timeÂ seriesÂ data,Â  theÂ proposed Â modelÂ notÂ onlyÂ accurately Â identi ï¬esÂ theÂ EPDÂ momentÂ butÂ alsoÂ captures Â theÂ  precursory Â symptoms Â ofÂ EPDÂ occurrence, Â enhancing Â theÂ reliability Â ofÂ theÂ system.Â InÂ additionÂ toÂ improving Â endpoint Â detection Â accuracy, Â theÂ proposed Â methodÂ alsoÂ reducedÂ theÂ  falseÂ detection Â rate,Â whichÂ canÂ negatively Â impactÂ yield.Â ThisÂ improvement Â canÂ contribute Â  toÂ enhanced Â EPDÂ accuracy Â inÂ high-aspect-ratio Â etchingÂ processes. Â Furthermore, Â theÂ proposedÂ LSTM-based Â methodÂ canÂ beÂ appliedÂ notÂ onlyÂ toÂ theÂ etchingÂ processÂ butÂ alsoÂ toÂ variousÂ stagesÂ ofÂ theÂ semiconductor Â manufacturing Â process,Â suchÂ asÂ cleaningÂ processes, Â processÂ monitoring, Â andÂ anomaly Â detection. Â ThisÂ canÂ beÂ utilizedÂ inÂ variousÂ ï¬eldsÂ whereÂ timeÂ  Figure 7. Attention-based LSTM neural network schematic. As shown in Table 8, the attention-based LSTM model improved the etch endpoint detection performance by approximately 1% and reduced the false detection rate (F1 score) by approximately 1% compared to the LSTM model. Table 8. Attention-based LSTM model confusion matrix metrics. Precision Recall F1 Score Without EPD 0.98 0.98 0.98 With EPD 0.98 0.98 0.98 Accuracy 0.98 4. Conclusions In this study, we proposed an LSTM-based method for detecting the endpoint in etching processes. Unlike traditional approaches that rely on signal patterns at specific time points, this method demonstrated its effectiveness by considering all the information from the beginning to the end of the etching process contained in the entire OES time series data, and the LSTM model achieved a high accuracy of 97.1%. The performance of the model was further improved by applying the attention mechanism to the LSTM model. The attention mechanism assigns higher weights to points in the time series data where the intensity change differs from the previous state, allowing the model to focus on the critical information for etch endpoint detection. This resulted in an approximately 1% performance improvement compared to the LSTM model, with the attention-based LSTM model ultimately achieving a high accuracy of 98.2%. The proposed attention-based LSTM model overcomes the limitations of the existing EPD algorithms and addresses the issues of traditional methods that primarily focus on signal changes only at the precise EPD moment. By utilizing the temporal dependencies inherent in the entire OES time series data, the proposed model not only accurately identifies the EPD moment but also captures the precursory symptoms of EPD occurrence, enhancing the reliability of the system. In addition to improving endpoint detection accuracy, the proposed method also reduced the false detection rate, which can negatively impact yield. This improvement can contribute to enhanced EPD accuracy in high-aspect-ratio etching processes. Furthermore, the proposed LSTM-based method can be applied not only to the etching process but also to various stages of the semiconductor manufacturing process, such as cleaning processes, process monitoring, and anomaly detection. This can be utilized in various fields where timeElectronics 2024 ,13, 3577 10 of 11 series data analysis plays a crucial role and is expected to contribute to the technological advancement of semiconductor manufacturing processes. Author Contributions: Conceptualization, S.J.H., Y.J.K. and J.H.S. (Jong Hyeon Shin); methodology, S.J.H., Y.J.K. and K.H.C.; software, Y.J.K. and J.H.S. (Jong Hyeon Shin); validation, J.S.K. and J.H.S. (Jung Ho Song); formal analysis, Y.J.K. and S.J.H.; investigation, Y.J.K., J.H.S. (Jong Hyeon Shin) and S.J.H.;\n",
      "\n",
      "Document 3:\n",
      "and C, and the peaks related to the reaction products in the reaction surface mechanism, CO, CN, and SiF. The wavelengths used can be found in Table 2. Table 2. Information about the wavelengths used for EPD. Species Wavelength (nm) SiF 336, 440 CO 482, 560, 561 CN 386, 387 F 685, 677, 703 C2 516 O 777, 844Electronics 2024 ,13, 3577 4 of 11 All coupon wafers were etched for a pre-determined fixed time. To ensure the complete etching of the SiO 2layer, over-etching was performed. During this process, it was confirmed through ellipsometry that approximately 200 nm of the Si 3N4layer was etched. This was carried out to verify that the SiO 2layer was fully etched. Therefore, the optical emission signals collected through OES serve as time series data spanning the entire etching process. By using these data as input for the LSTM model, the model can learn long-term dependencies and detect the etch endpoint. 3. LSTM Method The LSTM is a model designed to address the limitations of the RNN, specifically the problems of vanishing gradients and exploding gradients. The gradient information in the LSTM is more effective in maintaining and utilizing information over longer sequences compared to the RNN [27]. As shown in Figure 2, the LSTM consists of three gates: the input gate, forget gate, and output gate. Each gate has the following roles. The input gate determines how to update the internal state based on the current state and the previous hidden state [ 28]. It decides what information should be stored in the cell state. First, it determines what information to store from the current input and the previous hidden state through a sigmoid layer. Then, the hyperbolic tangent (tanh) layer generates a new candidate vector. The forget gate determines what information should be forgotten and to what extent, from the previous cell state, by passing it through a sigmoid layer. The output gate determines how much of the cell state should be reflected in the hidden state and decides the hidden state to be passed to the next time step of the LSTM [28]. Electronics Â 2024,Â 13,Â xÂ FORÂ PEERÂ REVIEW Â  4Â ofÂ 11Â  Â  Â AllÂ couponÂ wafersÂ wereÂ etchedÂ forÂ aÂ pre-determined Â ï¬xedÂ time.Â ToÂ ensureÂ theÂ completeÂ etchingÂ ofÂ theÂ SiO 2Â layer,Â over-etching Â wasÂ performed. Â DuringÂ thisÂ process,Â itÂ wasÂ  conï¬rmedÂ throughÂ ellipsometry Â thatÂ approximately Â 200Â nmÂ ofÂ theÂ Si3N4Â layerÂ wasÂ etched.Â  ThisÂ wasÂ carriedÂ outÂ toÂ verifyÂ thatÂ theÂ SiO 2Â layerÂ wasÂ fullyÂ etched.Â Therefore, Â theÂ opticalÂ  emission Â signalsÂ collected Â throughÂ OESÂ serveÂ asÂ timeÂ seriesÂ dataÂ spanning Â theÂ entireÂ etchingÂ process.Â ByÂ usingÂ theseÂ dataÂ asÂ inputÂ forÂ theÂ LSTMÂ model,Â theÂ modelÂ canÂ learnÂ longtermÂ dependencies Â andÂ detectÂ theÂ etchÂ endpoint. Â  3.Â LSTMÂ MethodÂ  TheÂ LSTMÂ isÂ aÂ modelÂ designed Â toÂ addressÂ theÂ limitations Â ofÂ theÂ RNN,Â speciï¬callyÂ theÂ  problems Â ofÂ vanishing Â gradients Â andÂ exploding Â gradients. Â TheÂ gradientÂ information Â inÂ theÂ  LSTMÂ isÂ moreÂ eï¬€ectiveÂ inÂ maintaining Â andÂ utilizingÂ information Â overÂ longerÂ sequences Â  compared Â toÂ theÂ RNNÂ [27].Â  AsÂ shownÂ inÂ FigureÂ 2,Â theÂ LSTMÂ consistsÂ ofÂ threeÂ gates:Â theÂ inputÂ gate,Â forgetÂ gate,Â  andÂ outputÂ gate.Â EachÂ gateÂ hasÂ theÂ following Â roles.Â TheÂ inputÂ gateÂ determines Â howÂ toÂ updateÂ theÂ internalÂ stateÂ basedÂ onÂ theÂ currentÂ stateÂ andÂ theÂ previous Â hiddenÂ stateÂ [28].Â ItÂ  decidesÂ whatÂ information Â shouldÂ beÂ storedÂ inÂ theÂ cellÂ state.Â First,Â itÂ determines Â whatÂ information Â toÂ storeÂ fromÂ theÂ currentÂ inputÂ andÂ theÂ previous Â hiddenÂ stateÂ throughÂ aÂ sigmoidÂ  layer.Â Then,Â theÂ hyperbolic Â tangentÂ (tanh)Â layerÂ generates Â aÂ newÂ candidate Â vector.Â TheÂ  forgetÂ gateÂ determines Â whatÂ information Â shouldÂ beÂ forgottenÂ andÂ toÂ whatÂ extent,Â fromÂ theÂ  previous Â cellÂ state,Â byÂ passingÂ itÂ throughÂ aÂ sigmoidÂ layer.Â TheÂ outputÂ gateÂ determines Â howÂ  muchÂ ofÂ theÂ cellÂ stateÂ shouldÂ beÂ reï¬‚ectedÂ inÂ theÂ hiddenÂ stateÂ andÂ decidesÂ theÂ hiddenÂ stateÂ  toÂ beÂ passedÂ toÂ theÂ nextÂ timeÂ stepÂ ofÂ theÂ LSTMÂ [28].Â  Â  FigureÂ 2.Â LSTMÂ diagramÂ [29].Â  TheÂ LSTMÂ controlsÂ theÂ cellÂ stateÂ usingÂ threeÂ gates.Â TheÂ cellÂ stateÂ containsÂ allÂ theÂ coreÂ  information, Â andÂ theÂ hiddenÂ stateÂ isÂ processed Â whenever Â necessary Â toÂ propagate Â informationÂ inÂ aÂ formÂ thatÂ exposesÂ onlyÂ theÂ required Â information Â forÂ eachÂ timeÂ step.Â TheÂ equationsÂ forÂ theÂ LSTMÂ areÂ asÂ follows:Â  ğ‘“à¯§àµŒ ğœáˆºğ‘Šà¯™âˆ™áˆ¾â„à¯§à¬¿à¬µ ,ğ‘¥à¯§áˆ¿àµ…ğ‘à¯™áˆ»,Â  (1) ğ‘–à¯§àµŒ ğœàµ«ğ‘Šà¯œâˆ™àµ£â„à¯§à¬¿à¬µ ,ğ‘¥à¯§àµ§àµ…ğ‘à¯œàµ¯,Â Â  (2) ğ¶âƒ—à¯§àµŒ tanháˆºğ‘Šà¯–âˆ™áˆ¾â„à¯§à¬¿à¬µ ,ğ‘¥à¯§áˆ¿àµ…ğ‘à¯–áˆ»,Â Â  (3) ğ¶à¯§àµŒ ğ‘“à¯§âˆ™ğ¶à¯§à¬¿à¬µàµ…ğ‘–à¯§âˆ™ ğ¶âƒ— à¯§,Â  (4) ğ‘œà¯§àµŒ ğœáˆºğ‘Šà¯¢âˆ™áˆ¾â„à¯§à¬¿à¬µ ,ğ‘¥à¯§áˆ¿àµ…ğ‘à¯¢áˆ»,Â  (5) â„à¯§àµŒ ğ‘œà¯œâˆ™tanh áˆºğ¶à¯§áˆ»Â  (6) Figure 2. LSTM diagram [29]. The LSTM controls the cell state using three gates. The cell state contains all the core information, and the hidden state is processed whenever necessary to propagate information in a form that exposes only the required information for each time step. The equations for the LSTM\n",
      "\n",
      "\n",
      "\n",
      "Question: How can attention mechanism be used for CD predictions?\n",
      "\n",
      "Answer: \n",
      "\n",
      "Using model: llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "# Ensure model is pulled\n",
    "print(f\"Ensuring Ollama model {OLLAMA_MODEL_NAME} is available...\")\n",
    "try:\n",
    "    ollama.pull(OLLAMA_MODEL_NAME)\n",
    "    print(f\"Model {OLLAMA_MODEL_NAME} is ready.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Error pulling model: {e.error}\")\n",
    "    print(\"You might need to install the model manually with: ollama pull \" + OLLAMA_MODEL_NAME)\n",
    "\n",
    "# Get prompt and model\n",
    "prompt, model_name = generate_response_with_ollama_2(query, results)\n",
    "print('\\n\\n\\n')\n",
    "print(prompt)\n",
    "print(f\"\\nUsing model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt created with 14490 characters\n",
      "First 200 characters of prompt:\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "model. The acquired OES data are character...\n"
     ]
    }
   ],
   "source": [
    "# Print prompt length\n",
    "print(f\"\\nPrompt created with {len(prompt)} characters\")\n",
    "print(\"First 200 characters of prompt:\")\n",
    "print(prompt[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response with Ollama...\n",
      "\n",
      "Response:\n",
      "Information not found.\n",
      "\n",
      "Response generation complete!\n",
      "Generated 22 characters\n"
     ]
    }
   ],
   "source": [
    "# Give promt as input to ollama generate function\n",
    "\n",
    "# Generate streaming response in Ollama: Setting stream=True when calling ollama.generate(...) \n",
    "# tells the Ollama client to return an iterator/stream of partial response chunks instead of waiting for the full answer. \n",
    "\n",
    "print(\"\\nGenerating response with Ollama...\")\n",
    "response = \"\"\n",
    "print(\"\\nResponse:\")\n",
    "for chunk in ollama.generate(model=model_name, prompt=prompt, stream=True, \n",
    "        system=\"You are a retrieval-grounded assistant. Use ONLY the provided context. \"\n",
    "           \"If the answer is not found, reply exactly: information not found.\",\n",
    "        options={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.0,\n",
    "        \"repeat_penalty\": 1.2,\n",
    "        \"num_ctx\": 8192\n",
    "    }):\n",
    "    piece = chunk['response']\n",
    "    print(piece, end='', flush=True)\n",
    "    response += piece\n",
    "\n",
    "print(\"\\n\\nResponse generation complete!\")\n",
    "print(f\"Generated {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other things that can be tried in this project\n",
    "- Use different embedding models (e.g., OpenAI, HuggingFace)\n",
    "\n",
    "\n",
    "## LLM prompting improvements\n",
    " - Instruction tuning / style\n",
    "â€œAnswer concisely using the context.â€ vs. â€œProvide a detailed technical explanation citing sources.â€\n",
    "\n",
    "- Chain-of-thought prompting\n",
    "Add reasoning steps explicitly:\n",
    "Think step by step using the provided context before answering.\n",
    "\n",
    "\n",
    "- Citation prompting\n",
    "Tell model to reference [source id] tokens.\n",
    "\n",
    "- Guardrails / hallucination control\n",
    "\n",
    "- Add explicit constraint:\n",
    "If the answer is not in the context, reply \"Information not found\".\n",
    "\n",
    "\n",
    "- Persona or role\n",
    "â€œYou are a semiconductor process expert explaining to an engineer.â€\n",
    "\n",
    "- Answer formatting\n",
    "Structured JSON or markdown summaries for downstream use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
