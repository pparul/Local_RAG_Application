{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hybrid Search and Retrieval with OpenSearch and Ollama\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Set up hybrid search with OpenSearch (combining text and semantic search)\n",
    "2. Generate embeddings for search queries\n",
    "3. Perform hybrid search to retrieve relevant document chunks\n",
    "4. Use Ollama to generate responses based on the retrieved context\n",
    "\n",
    "First, let's import the necessary dependencies and set up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parulpandey/Library/CloudStorage/OneDrive-Personal/Ext Github Repos/RAG_UI_fresh/.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up Python path to access project modules\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EMBEDDING_MODEL_PATH = \"sentence-transformers/all-mpnet-base-v2\"  # \n",
    "ASSYMETRIC_EMBEDDING = False  # Flag for asymmetric embedding\n",
    "EMBEDDING_DIMENSION = 768  # Embedding model settings\n",
    "TEXT_CHUNK_SIZE = 300  # Maximum number of characters in each text chunk for\n",
    "OLLAMA_MODEL_NAME = (\"llama3.2:1b\") # Name of the model used in Ollama for chat functionality\n",
    "\n",
    "# Logging\n",
    "LOG_FILE_PATH = \"logs/app.log\"  # File path for the application log file\n",
    "\n",
    "# OpenSearch settings\n",
    "OPENSEARCH_HOST = \"localhost\"  # Hostname for the OpenSearch instance\n",
    "OPENSEARCH_PORT = 9200  # Port number for OpenSearch\n",
    "OPENSEARCH_INDEX = \"documents\"  # Index name for storing documents in OpenSearch\n",
    "## Use http://localhost:9200\n",
    "# opensearch is runnibg on your local machine instead of a remote server\n",
    "# you have already started the opensearch container using docker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding settings\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for generating embeddings\n",
    "EMBEDDING_DIMENSION = 384  # Embedding dimension for the model\n",
    "ASSYMETRIC_EMBEDDING = False  # Whether to use asymmetric embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Connect to OpenSearch and Set Up Hybrid Search\n",
    "\n",
    "First, we'll connect to our OpenSearch instance and define the hybrid search function that combines:\n",
    "- Text-based search (BM25)\n",
    "- Vector-based semantic search (KNN)\n",
    "\n",
    "The hybrid search will use a pipeline that normalizes and combines scores from both search methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "# cleint talks to your host where opensearch is running. Gives tasks like indexing, searching, updating, deleting documents to the host. \n",
    "# An OpenSearch client is the official library (in Python, Java, JS, etc.) that wraps OpenSearchâ€™s REST API, so your app can connect, \n",
    "# query, and manage the cluster more easily.\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_compress=True,\n",
    "    timeout=30,\n",
    "    max_retries=3,\n",
    "    retry_on_timeout=True,\n",
    ")\n",
    "\n",
    "# Check connection\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"Successfully connected to OpenSearch {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to OpenSearch: {e}\")\n",
    "    print(\"Make sure OpenSearch is running on localhost:9200\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Search pipeline 'nlp-search-pipeline' exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify pipeline exists\n",
    "from opensearchpy.exceptions import NotFoundError\n",
    "pipeline_name = \"nlp-search-pipeline\"\n",
    "\n",
    "try:\n",
    "    result = client.transport.perform_request(\n",
    "        \"GET\",\n",
    "        f\"/_search/pipeline/{pipeline_name}\"\n",
    "    )\n",
    "    print(f\"\\nâœ… Search pipeline '{pipeline_name}' exists.\")\n",
    "    \n",
    "except NotFoundError:\n",
    "    print(f\"\\nâš ï¸ Search pipeline '{pipeline_name}' does NOT exist.\")\n",
    "    print(\"This is required for hybrid search. Please run the prerequisites notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nðŸš¨ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does open search combing query text and query embedding results \n",
    "- Since the range of bm25/knn is in different ranges they are normalized first \n",
    "normalized_bm25 = (bm25_score - min_bm25) / (max_bm25 - min_bm25)\n",
    "normalized_knn  = (knn_score - min_knn) / (max_knn - min_knn)\n",
    "final_score = w_text * normalized_bm25 + w_knn * normalized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a hybrid search for the query text using both text-based and vector-based search methods\n",
    "# Text-based search is effective for exact matches (BM25) and keyword relevance, while vector-based search captures semantic meaning (knn) \n",
    "# and context.\n",
    "# takes as input both query text and its embedding vector\n",
    "\n",
    "def hybrid_search(query_text: str, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search combining text-based and vector-based queries.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query for BM25 search\n",
    "        query_embedding (List[float]): The vector embedding for KNN search\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The search results\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"_source\": {\"exclude\": [\"embedding\"]},  # Exclude embeddings from results\n",
    "        \"query\": {\n",
    "            \"hybrid\": {\n",
    "                \"queries\": [\n",
    "                    {\"match\": {\"text\": {\"query\": query_text}}},  # Text-based search\n",
    "                    {\n",
    "                        \"knn\": {\n",
    "                            \"embedding\": {\n",
    "                                \"vector\": query_embedding,\n",
    "                                \"k\": top_k,\n",
    "                            }\n",
    "                        }\n",
    "                    },  # Vector-based search\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nExecuting hybrid search query...\")\n",
    "    try:\n",
    "        # Try with search pipeline parameter (for newer OpenSearch versions)\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body,\n",
    "            params={\"search_pipeline\": \"nlp-search-pipeline\"} # Uses the pipeline for score normalization\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fall back to without pipeline parameter for older versions\n",
    "        print(\"Warning: OpenSearch client doesn't support search_pipeline parameter, using raw query\")\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body\n",
    "        )\n",
    "    \n",
    "    return response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rescore query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'knn': {'embedding': {'vector': [0.1, 0.4, 0.6], 'k': 100}}},\n",
       " 'rescore': {'window_size': 50,\n",
       "  'query': {'rescore_query': {'match': {'text': 'quantum computers'}},\n",
       "   'query_weight': 0.3,\n",
       "   'rescore_query_weight': 0.7}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Combines k-NN search with a text-based rescore query to refine results\n",
    "# The k-NN search retrieves the top 100 nearest neighbors based on the embedding vector\n",
    "# The rescore query then re-evaluates the top 50 results using a text match\n",
    "# The final ranking is a weighted combination of the original k-NN scores and the rescore text match scores\n",
    "\n",
    "## rescore_query_weight tells OpenSearch how much to trust the rescore query relative to the original one.\n",
    "\n",
    "{\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"embedding\": {\n",
    "        \"vector\": [0.1, 0.4, 0.6],\n",
    "        \"k\": 100\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"rescore\": {\n",
    "    \"window_size\": 50,\n",
    "    \"query\": {\n",
    "      \"rescore_query\": { \"match\": { \"text\": \"quantum computers\" } },\n",
    "      \"query_weight\": 0.3,\n",
    "      \"rescore_query_weight\": 0.7\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Process Query and Perform Search\n",
    "\n",
    "Now we'll demonstrate how to:\n",
    "1. Process a search query\n",
    "2. Generate its embedding\n",
    "3. Perform hybrid search to get relevant document chunks\n",
    "\n",
    "Let's try a sample query to test our search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Loads and returns the sentence transformer embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text chunks to embed.\n",
    "        \n",
    "    Returns:\n",
    "        List[numpy.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    model = get_embedding_model()\n",
    "    \n",
    "    # If using asymmetric embeddings, prefix each text with \"passage: \"\n",
    "    if ASSYMETRIC_EMBEDDING:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is the average rate of ice loss'\n",
      "\n",
      "Generating embedding for query...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Generated embedding with dimension: 384\n",
      "\n",
      "Retrieving top 3 documents...\n",
      "\n",
      "Executing hybrid search query...\n",
      "\n",
      "Search results for query: 'What is the average rate of ice loss'\n",
      "\n",
      "Result 1 (Score: 0.700):\n",
      "Text: Gt yrâˆ’1 of ice loss is equivalent to about 0.28 mm yrâˆ’1 of global mean sea level rise.SPMSummary for Policymakers101900 1920 1940 1960 1980 2000âˆ’20âˆ’1001020 Year (1022 J)Change in global average upper ...\n",
      "Document: climate\n",
      "\n",
      "Result 2 (Score: 0.543):\n",
      "Text: km2 per decade), and very likely in the range 9.4 to 13.6% per decade (range of 0.73 to 1.07 million km2 per decade) for the summer sea ice minimum (perennial sea ice). The average decrease in decadal...\n",
      "Document: climate\n",
      "\n",
      "Result 3 (Score: 0.300):\n",
      "Text: evaporation and precipitation over the oceans have changed ( medium confidence ). {2.5, 3.3, 3.5} â€¢ There is no observational evidence of a trend in the Atlantic Meridional Overturning Circulation (AM...\n",
      "Document: climate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "query = \"What is the average rate of ice loss\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "# Generate query embedding\n",
    "print(\"\\nGenerating embedding for query...\")\n",
    "embeddings = generate_embeddings([query])\n",
    "query_embedding = embeddings[0].tolist()\n",
    "print(f\"Generated embedding with dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Set number of results to retrieve\n",
    "top_k = 3\n",
    "print(f\"\\nRetrieving top {top_k} documents...\")\n",
    "\n",
    "# Perform hybrid search\n",
    "results = hybrid_search(query, query_embedding, top_k=top_k)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSearch results for query: '{query}'\\n\")\n",
    "for i, hit in enumerate(results, 1):\n",
    "    print(f\"Result {i} (Score: {hit['_score']:.3f}):\")\n",
    "    print(f\"Text: {hit['_source']['text'][:200]}...\")  # Showing truncated text\n",
    "    print(f\"Document: {hit['_source']['document_name']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Generate Response with Ollama\n",
    "\n",
    "Finally, we'll use Ollama to generate a response based on the retrieved context. We'll:\n",
    "1. Format the context and query into a prompt\n",
    "2. Stream the response from Ollama\n",
    "3. Display the generated response\n",
    "\n",
    "Make sure you have Ollama running locally with the specified model pulled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate responses with Ollama\n",
    "def generate_response_with_ollama(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Generates a response using Ollama based on search results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        results (List[Dict]): The search results from OpenSearch\n",
    "        model_name (str): The Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing (prompt, model_name)\n",
    "    \"\"\"\n",
    "    # Format context from search results\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "Think step by step using the provided context before answering. If you cannot find the answer in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    return prompt, model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring Ollama model llama3.2:1b is available...\n",
      "Model llama3.2:1b is ready.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "Think step by step using the provided context before answering. If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "Gt yrâˆ’1 of ice loss is equivalent to about 0.28 mm yrâˆ’1 of global mean sea level rise.SPMSummary for Policymakers101900 1920 1940 1960 1980 2000âˆ’20âˆ’1001020 Year (1022 J)Change in global average upper ocean heat content (c) Global average sea level change 1900 1920 1940 1960 1980 2000âˆ’50050100150200 Year(mm)(d)Arctic summer sea ice extent 1900 1920 1940 1960 1980 2000468101214 Year(million km2)(b)Northern Hemisphere spring snow cover 1900 1920 1940 1960 1980 200030354045 Year(million km2)(a) Figure SPM.3 | Multiple observed indicators of a changing global climate: (a) Extent of Northern Hemisphere March-April (spring) average snow cover; (b) extent of Arctic July-August-September (summer) average sea ice; (c) change in global mean upper ocean (0â€“700 m) heat content aligned to 2006âˆ’2010, and relative to the mean of all datasets for 1970; (d) global mean sea level relative to the 1900â€“1905 mean of the longest running dataset, and with all datasets aligned to have the same value in 1993, the first year of satellite altimetry data. All time-series (coloured lines indicating different data sets) show annual values, and where assessed, uncertainties are indicated by coloured shading. See Technical Summary Supplementary Material for a listing of the datasets. {Figures 3.2, 3.13, 4.19, and 4.3; FAQ 2.1, Figure 2; Figure TS.1}SPM Summary for Policymakers11B.4 Sea Level The atmospheric concentrations of carbon dioxide, methane, and nitrous oxide have increased to levels unprecedented in at least the last 800,000 years. Carbon dioxide concentrations have increased by 40% since pre-industrial times, primarily from fossil fuel emissions and secondarily from net land use change emissions. The ocean has absorbed about 30% of the emitted anthropogenic carbon dioxide, causing ocean acidification (see Figure SPM.4). {2.2, 3.8, 5.2, 6.2, 6.3} 11 ppm (parts per million) or ppb (parts per billion, 1 billion = 1,000 million) is the ratio of the number of gas molecules to the total number of molecules of dry air. For example, 300 ppm means 300 molecules of a gas per million molecules of dry air.The rate of sea level rise since the mid-19th century has been larger than the mean rate during the previous two millennia ( high confidence ). Over the period 1901 to 2010, global mean sea level rose by 0.19 [0.17 to 0.21] m (see Figure SPM.3). {3.7, 5.6, 13.2} â€¢ Proxy and instrumental sea level data indicate a transition in the late 19th to the early 20th century from relatively low mean rates of rise over the previous two millennia to higher rates of rise ( high confidence ). It is likely that the rate of global mean sea level rise has continued to increase since the early 20th century. {3.7, 5.6, 13.2} â€¢ It is very likely that the mean rate of global averaged sea level rise was 1.7 [1.5 to 1.9] mm yrâ€“1 between 1901 and 2010, 2.0 [1.7 to 2.3] mm yrâ€“1 between 1971 and 2010, and 3.2 [2.8 to 3.6] mm yrâ€“1 between 1993 and 2010. Tide-gauge and satellite altimeter data are consistent regarding the higher rate of the latter period. It\n",
      "\n",
      "Document 2:\n",
      "km2 per decade), and very likely in the range 9.4 to 13.6% per decade (range of 0.73 to 1.07 million km2 per decade) for the summer sea ice minimum (perennial sea ice). The average decrease in decadal mean extent of Arctic sea ice has been most rapid in summer ( high confidence ); the spatial extent has decreased in every season, and in every successive decade since 1979 ( high confidence ) (see Figure SPM.3). There is medium confidence from reconstructions that over the past three decades, Arctic summer sea ice retreat was unprecedented and sea surface temperatures were anomalously high in at least the last 1,450 years. {4.2, 5.5} â€¢ It is very likely that the annual mean Antarctic sea ice extent increased at a rate in the range of 1.2 to 1.8% per decade (range of 0.13 to 0.20 million km2 per decade) between 1979 and 2012. There is high confidence that there are strong regional differences in this annual rate, with extent increasing in some regions and decreasing in others. {4.2} â€¢ There is very high confidence that the extent of Northern Hemisphere snow cover has decreased since the mid-20th century (see Figure SPM.3). Northern Hemisphere snow cover extent decreased 1.6 [0.8 to 2.4] % per decade for March and April, and 11.7 [8.8 to 14.6] % per decade for June, over the 1967 to 2012 period. During this period, snow cover extent in the Northern Hemisphere did not show a statistically significant increase in any month. {4.5} â€¢ There is high confidence that permafrost temperatures have increased in most regions since the early 1980s. Observed warming was up to 3Â°C in parts of Northern Alaska (early 1980s to mid-2000s) and up to 2Â°C in parts of the Russian European North (1971 to 2010). In the latter region, a considerable reduction in permafrost thickness and areal extent has been observed over the period 1975 to 2005 ( medium confidence ). {4.7} â€¢ Multiple lines of evidence support very substantial Arctic warming since the mid-20th century. {Box 5.1, 10.3} 8 All references to â€˜ice lossâ€™ or â€˜mass lossâ€™ refer to net ice loss, i.e., accumulation minus melt and iceberg calving. 9 For methodological reasons, this assessment of ice loss from the Antarctic and Greenland ice sheets includes change in the glaciers on the periphery. These peripheral glaciers are thus excluded from the values given for glaciers. 10 100 Gt yrâˆ’1 of ice loss is equivalent to about 0.28 mm yrâˆ’1 of global mean sea level rise.SPMSummary for Policymakers101900 1920 1940 1960 1980 2000âˆ’20âˆ’1001020 Year (1022 J)Change in global average upper ocean heat content (c) Global average sea level change 1900 1920 1940 1960 1980 2000âˆ’50050100150200 Year(mm)(d)Arctic summer sea ice extent 1900 1920 1940 1960 1980 2000468101214 Year(million km2)(b)Northern Hemisphere spring snow cover 1900 1920 1940 1960 1980 200030354045 Year(million km2)(a) Figure SPM.3 | Multiple observed indicators of a changing global climate: (a) Extent of Northern Hemisphere March-April (spring) average snow cover; (b) extent of Arctic July-August-September (summer) average\n",
      "\n",
      "Document 3:\n",
      "evaporation and precipitation over the oceans have changed ( medium confidence ). {2.5, 3.3, 3.5} â€¢ There is no observational evidence of a trend in the Atlantic Meridional Overturning Circulation (AMOC), based on the decade-long record of the complete AMOC and longer records of individual AMOC components. {3.6} Figure SPM.2 | Maps of observed precipitation change from 1901 to 2010 and from 1951 to 2010 (trends in annual accumulation calculated using the same criteria as in Figure SPM.1) from one data set. For further technical details see the Technical Summary Supplementary Material. {TS TFE.1, Figure 2; Figure 2.29} âˆ’100 âˆ’50 âˆ’25 âˆ’10 âˆ’5 âˆ’2.5 0 2.5 51 02 55 0 100 (mm yr-1 per decade)1901â€“ 2010 1951â€“ 2010Observed change in annual precipitation over land 7 A constant supply of heat through the ocean surface at the rate of 1 W mâ€“2 for 1 year would increase the ocean heat content by 1.1 Ã— 1022 J.SPM Summary for Policymakers9B.3 Cryosphere Over the last two decades, the Greenland and Antarctic ice sheets have been losing mass, glaciers have continued to shrink almost worldwide, and Arctic sea ice and Northern Hemisphere spring snow cover have continued to decrease in extent ( high confidence ) (see Figure SPM.3). {4.2â€“4.7} â€¢ The average rate of ice loss8 from glaciers around the world, excluding glaciers on the periphery of the ice sheets9, was very likely 226 [91 to 361] Gt yrâˆ’1 over the period 1971 to 2009, and very likely 275 [140 to 410] Gt yrâˆ’1 over the period 1993 to 200910. {4.3} â€¢ The average rate of ice loss from the Greenland ice sheet has very likely substantially increased from 34 [â€“6 to 74] Gt yrâ€“1 over the period 1992 to 2001 to 215 [157 to 274] Gt yrâ€“1 over the period 2002 to 2011. {4.4} â€¢ The average rate of ice loss from the Antarctic ice sheet has likely increased from 30 [â€“37 to 97] Gt yrâ€“1 over the period 1992â€“2001 to 147 [72 to 221] Gt yrâ€“1 over the period 2002 to 2011. There is very high confidence that these losses are mainly from the northern Antarctic Peninsula and the Amundsen Sea sector of West Antarctica. {4.4} â€¢ The annual mean Arctic sea ice extent decreased over the period 1979 to 2012 with a rate that was very likely in the range 3.5 to 4.1% per decade (range of 0.45 to 0.51 million km2 per decade), and very likely in the range 9.4 to 13.6% per decade (range of 0.73 to 1.07 million km2 per decade) for the summer sea ice minimum (perennial sea ice). The average decrease in decadal mean extent of Arctic sea ice has been most rapid in summer ( high confidence ); the spatial extent has decreased in every season, and in every successive decade since 1979 ( high confidence ) (see Figure SPM.3). There is medium confidence from reconstructions that over the past three decades, Arctic summer sea ice retreat was unprecedented and sea surface temperatures were anomalously\n",
      "\n",
      "\n",
      "\n",
      "Question: What is the average rate of ice loss\n",
      "\n",
      "Answer: \n",
      "\n",
      "Using model: llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "# Ensure model is pulled\n",
    "print(f\"Ensuring Ollama model {OLLAMA_MODEL_NAME} is available...\")\n",
    "try:\n",
    "    ollama.pull(OLLAMA_MODEL_NAME)\n",
    "    print(f\"Model {OLLAMA_MODEL_NAME} is ready.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Error pulling model: {e.error}\")\n",
    "    print(\"You might need to install the model manually with: ollama pull \" + OLLAMA_MODEL_NAME)\n",
    "\n",
    "# Get prompt and model\n",
    "prompt, model_name = generate_response_with_ollama(query, results)\n",
    "print('\\n\\n\\n')\n",
    "print(prompt)\n",
    "print(f\"\\nUsing model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt created with 9253 characters\n",
      "First 200 characters of prompt:\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "Think step by step using the provided context before answering. If you cannot find the answer in the context, say so.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Print prompt length\n",
    "print(f\"\\nPrompt created with {len(prompt)} characters\")\n",
    "print(\"First 200 characters of prompt:\")\n",
    "print(prompt[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response with Ollama...\n",
      "\n",
      "Response:\n",
      "Based on the provided context, the question asks for the average rate of ice loss. However, there are different rates mentioned for various components:\n",
      "\n",
      "- The Greenland ice sheet has an average rate of ice loss of very likely 226 Gt yrâˆ’1 (1971-2009) or 275 Gt yrâˆ’1 (1993-2001).\n",
      "- The Antarctic ice sheet has an average rate of ice loss that is still being studied, but it's mentioned as \"likely\" to be in the range of 30-97 Gt yrâˆ’1.\n",
      "- Arctic sea ice and Northern Hemisphere spring snow cover have both decreased in extent over time.\n",
      "\n",
      "To find a single average rate for all these components combined, we need to look at various studies and datasets that provide estimates for each component. However, without direct comparison or a specific dataset mentioned in the context provided, it's challenging to accurately calculate an average rate of ice loss.\n",
      "\n",
      "Given the complexity and variability across different sources and time periods, I would be unable to accurately determine a single average rate of ice loss from the information provided.\n",
      "\n",
      "Response generation complete!\n",
      "Generated 1039 characters\n"
     ]
    }
   ],
   "source": [
    "# Give promt as input to ollama generate function\n",
    "\n",
    "# Generate streaming response in Ollama: Setting stream=True when calling ollama.generate(...) \n",
    "# tells the Ollama client to return an iterator/stream of partial response chunks instead of waiting for the full answer. \n",
    "\n",
    "print(\"\\nGenerating response with Ollama...\")\n",
    "response = \"\"\n",
    "print(\"\\nResponse:\")\n",
    "for chunk in ollama.generate(model=model_name, prompt=prompt, stream=True):\n",
    "    piece = chunk['response']\n",
    "    print(piece, end='', flush=True)\n",
    "    response += piece\n",
    "\n",
    "print(\"\\n\\nResponse generation complete!\")\n",
    "print(f\"Generated {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. Connected to OpenSearch and verified the hybrid search pipeline\n",
    "2. Generated embeddings for a search query\n",
    "3. Performed hybrid search combining BM25 and semantic search\n",
    "4. Generated a response using Ollama based on the retrieved documents\n",
    "\n",
    "You can experiment with different:\n",
    "- Search queries\n",
    "- Hybrid search parameters and weights\n",
    "- Ollama models and prompts\n",
    "- Result formatting and processing\n",
    "\n",
    "This completes the basic RAG (Retrieval Augmented Generation) pipeline that combines the power of OpenSearch for hybrid retrieval with the generation capabilities of LLMs through Ollama."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
