{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hybrid Search and Retrieval with OpenSearch and Ollama\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Set up hybrid search with OpenSearch (combining text and semantic search)\n",
    "2. Generate embeddings for search queries\n",
    "3. Perform hybrid search to retrieve relevant document chunks\n",
    "4. Use Ollama to generate responses based on the retrieved context\n",
    "\n",
    "First, let's import the necessary dependencies and set up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parulpandey/Library/CloudStorage/OneDrive-Personal/Ext Github Repos/RAG_UI_fresh/.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up Python path to access project modules\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EMBEDDING_MODEL_PATH = \"sentence-transformers/all-mpnet-base-v2\"  # \n",
    "ASSYMETRIC_EMBEDDING = False  # Flag for asymmetric embedding\n",
    "EMBEDDING_DIMENSION = 768  # Embedding model settings\n",
    "TEXT_CHUNK_SIZE = 300  # Maximum number of characters in each text chunk for\n",
    "OLLAMA_MODEL_NAME = (\"llama3.2:1b\") # Name of the model used in Ollama for chat functionality\n",
    "\n",
    "# Logging\n",
    "LOG_FILE_PATH = \"logs/app.log\"  # File path for the application log file\n",
    "\n",
    "# OpenSearch settings\n",
    "OPENSEARCH_HOST = \"localhost\"  # Hostname for the OpenSearch instance\n",
    "OPENSEARCH_PORT = 9200  # Port number for OpenSearch\n",
    "OPENSEARCH_INDEX = \"documents\"  # Index name for storing documents in OpenSearch\n",
    "## Use http://localhost:9200\n",
    "# opensearch is runnibg on your local machine instead of a remote server\n",
    "# you have already started the opensearch container using docker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding settings\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for generating embeddings\n",
    "EMBEDDING_DIMENSION = 384  # Embedding dimension for the model\n",
    "ASSYMETRIC_EMBEDDING = False  # Whether to use asymmetric embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Connect to OpenSearch and Set Up Hybrid Search\n",
    "\n",
    "First, we'll connect to our OpenSearch instance and define the hybrid search function that combines:\n",
    "- Text-based search (BM25)\n",
    "- Vector-based semantic search (KNN)\n",
    "\n",
    "The hybrid search will use a pipeline that normalizes and combines scores from both search methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "# cleint talks to your host where opensearch is running. Gives tasks like indexing, searching, updating, deleting documents to the host. \n",
    "# An OpenSearch client is the official library (in Python, Java, JS, etc.) that wraps OpenSearchâ€™s REST API, so your app can connect, \n",
    "# query, and manage the cluster more easily.\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_compress=True,\n",
    "    timeout=30,\n",
    "    max_retries=3,\n",
    "    retry_on_timeout=True,\n",
    ")\n",
    "\n",
    "# Check connection\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"Successfully connected to OpenSearch {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to OpenSearch: {e}\")\n",
    "    print(\"Make sure OpenSearch is running on localhost:9200\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Search pipeline 'nlp-search-pipeline' exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify pipeline exists\n",
    "from opensearchpy.exceptions import NotFoundError\n",
    "pipeline_name = \"nlp-search-pipeline\"\n",
    "\n",
    "try:\n",
    "    result = client.transport.perform_request(\n",
    "        \"GET\",\n",
    "        f\"/_search/pipeline/{pipeline_name}\"\n",
    "    )\n",
    "    print(f\"\\nâœ… Search pipeline '{pipeline_name}' exists.\")\n",
    "    \n",
    "except NotFoundError:\n",
    "    print(f\"\\nâš ï¸ Search pipeline '{pipeline_name}' does NOT exist.\")\n",
    "    print(\"This is required for hybrid search. Please run the prerequisites notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nðŸš¨ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does open search combing query text and query embedding results \n",
    "- Since the range of bm25/knn is in different ranges they are normalized first \n",
    "normalized_bm25 = (bm25_score - min_bm25) / (max_bm25 - min_bm25)\n",
    "normalized_knn  = (knn_score - min_knn) / (max_knn - min_knn)\n",
    "final_score = w_text * normalized_bm25 + w_knn * normalized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a hybrid search for the query text using both text-based and vector-based search methods\n",
    "# Text-based search is effective for exact matches (BM25) and keyword relevance, while vector-based search captures semantic meaning (knn) \n",
    "# and context.\n",
    "# takes as input both query text and its embedding vector\n",
    "\n",
    "def hybrid_search(query_text: str, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search combining text-based and vector-based queries.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query for BM25 search\n",
    "        query_embedding (List[float]): The vector embedding for KNN search\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The search results\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"_source\": {\"exclude\": [\"embedding\"]},  # Exclude embeddings from results\n",
    "        \"query\": {\n",
    "            \"hybrid\": {\n",
    "                \"queries\": [\n",
    "                    {\"match\": {\"text\": {\"query\": query_text}}},  # Text-based search\n",
    "                    {\n",
    "                        \"knn\": {\n",
    "                            \"embedding\": {\n",
    "                                \"vector\": query_embedding,\n",
    "                                \"k\": top_k,\n",
    "                            }\n",
    "                        }\n",
    "                    },  # Vector-based search\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nExecuting hybrid search query...\")\n",
    "    try:\n",
    "        # Try with search pipeline parameter (for newer OpenSearch versions)\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body,\n",
    "            params={\"search_pipeline\": \"nlp-search-pipeline\"} # Uses the pipeline for score normalization\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fall back to without pipeline parameter for older versions\n",
    "        print(\"Warning: OpenSearch client doesn't support search_pipeline parameter, using raw query\")\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body\n",
    "        )\n",
    "    \n",
    "    return response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rescore query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'knn': {'embedding': {'vector': [0.1, 0.4, 0.6], 'k': 100}}},\n",
       " 'rescore': {'window_size': 50,\n",
       "  'query': {'rescore_query': {'match': {'text': 'quantum computers'}},\n",
       "   'query_weight': 0.3,\n",
       "   'rescore_query_weight': 0.7}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Combines k-NN search with a text-based rescore query to refine results\n",
    "# The k-NN search retrieves the top 100 nearest neighbors based on the embedding vector\n",
    "# The rescore query then re-evaluates the top 50 results using a text match\n",
    "# The final ranking is a weighted combination of the original k-NN scores and the rescore text match scores\n",
    "\n",
    "## rescore_query_weight tells OpenSearch how much to trust the rescore query relative to the original one.\n",
    "\n",
    "{\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"embedding\": {\n",
    "        \"vector\": [0.1, 0.4, 0.6],\n",
    "        \"k\": 100\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"rescore\": {\n",
    "    \"window_size\": 50,\n",
    "    \"query\": {\n",
    "      \"rescore_query\": { \"match\": { \"text\": \"quantum computers\" } },\n",
    "      \"query_weight\": 0.3,\n",
    "      \"rescore_query_weight\": 0.7\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Process Query and Perform Search\n",
    "\n",
    "Now we'll demonstrate how to:\n",
    "1. Process a search query\n",
    "2. Generate its embedding\n",
    "3. Perform hybrid search to get relevant document chunks\n",
    "\n",
    "Let's try a sample query to test our search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Loads and returns the sentence transformer embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text chunks to embed.\n",
    "        \n",
    "    Returns:\n",
    "        List[numpy.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    model = get_embedding_model()\n",
    "    \n",
    "    # If using asymmetric embeddings, prefix each text with \"passage: \"\n",
    "    if ASSYMETRIC_EMBEDDING:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is an ice creame'\n",
      "\n",
      "Generating embedding for query...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Generated embedding with dimension: 384\n",
      "\n",
      "Retrieving top 3 documents...\n",
      "\n",
      "Executing hybrid search query...\n",
      "\n",
      "Search results for query: 'What is an ice creame'\n",
      "\n",
      "Result 1 (Score: 0.700):\n",
      "Text: ). These ranges are derived from CMIP5 climate projections in combination with process-based models and literature assessment of glacier and ice sheet contributions (see Figure SPM.9, Table SPM.2). {1...\n",
      "Document: climate\n",
      "\n",
      "Result 2 (Score: 0.617):\n",
      "Text: km2 per decade), and very likely in the range 9.4 to 13.6% per decade (range of 0.73 to 1.07 million km2 per decade) for the summer sea ice minimum (perennial sea ice). The average decrease in decadal...\n",
      "Document: climate\n",
      "\n",
      "Result 3 (Score: 0.300):\n",
      "Text: the surface to the deep ocean and affect ocean circulation. {11.3, 12.4} It is very likely that the Arctic sea ice cover will continue to shrink and thin and that Northern Hemisphere spring snow cover...\n",
      "Document: climate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "#query = \"What is the average rate of ice loss\"\n",
    "query = \"What is an ice creame\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "# Generate query embedding\n",
    "print(\"\\nGenerating embedding for query...\")\n",
    "embeddings = generate_embeddings([query])\n",
    "query_embedding = embeddings[0].tolist()\n",
    "print(f\"Generated embedding with dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Set number of results to retrieve\n",
    "top_k = 3\n",
    "print(f\"\\nRetrieving top {top_k} documents...\")\n",
    "\n",
    "# Perform hybrid search\n",
    "results = hybrid_search(query, query_embedding, top_k=top_k)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSearch results for query: '{query}'\\n\")\n",
    "for i, hit in enumerate(results, 1):\n",
    "    print(f\"Result {i} (Score: {hit['_score']:.3f}):\")\n",
    "    print(f\"Text: {hit['_source']['text'][:200]}...\")  # Showing truncated text\n",
    "    print(f\"Document: {hit['_source']['document_name']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Generate Response with Ollama\n",
    "\n",
    "Finally, we'll use Ollama to generate a response based on the retrieved context. We'll:\n",
    "1. Format the context and query into a prompt\n",
    "2. Stream the response from Ollama\n",
    "3. Display the generated response\n",
    "\n",
    "Make sure you have Ollama running locally with the specified model pulled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate responses with Ollama\n",
    "def generate_response_with_ollama(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Generates a response using Ollama based on search results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        results (List[Dict]): The search results from OpenSearch\n",
    "        model_name (str): The Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing (prompt, model_name)\n",
    "    \"\"\"\n",
    "    # Format context from search results\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you cannot find the answer in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    return prompt, model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring Ollama model llama3.2:1b is available...\n",
      "Model llama3.2:1b is ready.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "). These ranges are derived from CMIP5 climate projections in combination with process-based models and literature assessment of glacier and ice sheet contributions (see Figure SPM.9, Table SPM.2). {13.5} â€¢ In the RCP projections, thermal expansion accounts for 30 to 55% of 21st century global mean sea level rise, and glaciers for 15 to 35%. The increase in surface melting of the Greenland ice sheet will exceed the increase in snowfall, leading to a positive contribution from changes in surface mass balance to future sea level ( high confidence ). While surface melt - ing will remain small, an increase in snowfall on the Antarctic ice sheet is expected ( medium confidence ), resulting in a negative contribution to future sea level from changes in surface mass balance. Changes in outflow from both ice sheets combined will likely make a contribution in the range of 0.03 to 0.20 m by 2081âˆ’2100 ( medium confidence ). {13.3âˆ’13.5} â€¢ Based on current understanding, only the collapse of marine-based sectors of the Antarctic ice sheet, if initiated, could cause global mean sea level to rise substantially above the likely range during the 21st century. However, there is medium confidence that this additional contribution would not exceed several tenths of a meter of sea level rise during the 21st century. {13.4, 13.5}SPMSummary for Policymakers26â€¢ The basis for higher projections of global mean sea level rise in the 21st century has been considered and it has been concluded that there is currently insufficient evidence to evaluate the probability of specific levels above the assessed likely range. Many semi-empirical model projections of global mean sea level rise are higher than process-based model projections (up to about twice as large), but there is no consensus in the scientific community about their reliability and there is thus low confidence in their projections. {13.5} â€¢ Sea level rise will not be uniform. By the end of the 21st century, it is very likely that sea level will rise in more than about 95% of the ocean area. About 70% of the coastlines worldwide are projected to experience sea level change within 20% of the global mean sea level change. {13.1, 13.6} E.7 Carbon and Other Biogeochemical Cycles Climate change will affect carbon cycle processes in a way that will exacerbate the increase of CO2 in the atmosphere ( high confidence ). Further uptake of carbon by the ocean will increase ocean acidification. {6.4} â€¢ Ocean uptake of anthropogenic CO2 will continue under all four RCPs through to 2100, with higher uptake for higher concentration pathways ( very high confidence ). The future evolution of the land carbon uptake is less certain. A majority of models projects a continued land carbon uptake under all RCPs, but some models simulate a land carbon loss due to the combined effect of climate change and land use change. {6.4} â€¢ Based on Earth System Models, there is high confidence that the feedback between climate and the carbon cycle is positive in the 21st\n",
      "\n",
      "Document 2:\n",
      "km2 per decade), and very likely in the range 9.4 to 13.6% per decade (range of 0.73 to 1.07 million km2 per decade) for the summer sea ice minimum (perennial sea ice). The average decrease in decadal mean extent of Arctic sea ice has been most rapid in summer ( high confidence ); the spatial extent has decreased in every season, and in every successive decade since 1979 ( high confidence ) (see Figure SPM.3). There is medium confidence from reconstructions that over the past three decades, Arctic summer sea ice retreat was unprecedented and sea surface temperatures were anomalously high in at least the last 1,450 years. {4.2, 5.5} â€¢ It is very likely that the annual mean Antarctic sea ice extent increased at a rate in the range of 1.2 to 1.8% per decade (range of 0.13 to 0.20 million km2 per decade) between 1979 and 2012. There is high confidence that there are strong regional differences in this annual rate, with extent increasing in some regions and decreasing in others. {4.2} â€¢ There is very high confidence that the extent of Northern Hemisphere snow cover has decreased since the mid-20th century (see Figure SPM.3). Northern Hemisphere snow cover extent decreased 1.6 [0.8 to 2.4] % per decade for March and April, and 11.7 [8.8 to 14.6] % per decade for June, over the 1967 to 2012 period. During this period, snow cover extent in the Northern Hemisphere did not show a statistically significant increase in any month. {4.5} â€¢ There is high confidence that permafrost temperatures have increased in most regions since the early 1980s. Observed warming was up to 3Â°C in parts of Northern Alaska (early 1980s to mid-2000s) and up to 2Â°C in parts of the Russian European North (1971 to 2010). In the latter region, a considerable reduction in permafrost thickness and areal extent has been observed over the period 1975 to 2005 ( medium confidence ). {4.7} â€¢ Multiple lines of evidence support very substantial Arctic warming since the mid-20th century. {Box 5.1, 10.3} 8 All references to â€˜ice lossâ€™ or â€˜mass lossâ€™ refer to net ice loss, i.e., accumulation minus melt and iceberg calving. 9 For methodological reasons, this assessment of ice loss from the Antarctic and Greenland ice sheets includes change in the glaciers on the periphery. These peripheral glaciers are thus excluded from the values given for glaciers. 10 100 Gt yrâˆ’1 of ice loss is equivalent to about 0.28 mm yrâˆ’1 of global mean sea level rise.SPMSummary for Policymakers101900 1920 1940 1960 1980 2000âˆ’20âˆ’1001020 Year (1022 J)Change in global average upper ocean heat content (c) Global average sea level change 1900 1920 1940 1960 1980 2000âˆ’50050100150200 Year(mm)(d)Arctic summer sea ice extent 1900 1920 1940 1960 1980 2000468101214 Year(million km2)(b)Northern Hemisphere spring snow cover 1900 1920 1940 1960 1980 200030354045 Year(million km2)(a) Figure SPM.3 | Multiple observed indicators of a changing global climate: (a) Extent of Northern Hemisphere March-April (spring) average snow cover; (b) extent of Arctic July-August-September (summer) average\n",
      "\n",
      "Document 3:\n",
      "the surface to the deep ocean and affect ocean circulation. {11.3, 12.4} It is very likely that the Arctic sea ice cover will continue to shrink and thin and that Northern Hemisphere spring snow cover will decrease during the 21st century as global mean surface temperature rises. Global glacier volume will further decrease. {12.4, 13.4}â€¢ The strongest ocean warming is projected for the surface in tropical and Northern Hemisphere subtropical regions. At greater depth the warming will be most pronounced in the Southern Ocean ( high confidence ). Best estimates of ocean warming in the top one hundred meters are about 0.6Â°C (RCP2.6) to 2.0Â°C (RCP8.5), and about 0.3Â°C (RCP2.6) to 0.6Â°C (RCP8.5) at a depth of about 1000 m by the end of the 21st century. {12.4, 14.3} â€¢ It is very likely that the Atlantic Meridional Overturning Circulation (AMOC) will weaken over the 21st century. Best estimates and ranges18 for the reduction are 11% (1 to 24%) in RCP2.6 and 34% (12 to 54%) in RCP8.5. It is likely that there will be some decline in the AMOC by about 2050, but there may be some decades when the AMOC increases due to large natural internal variability. {11.3, 12.4} â€¢ It is very unlikely that the AMOC will undergo an abrupt transition or collapse in the 21st century for the scenarios considered. There is low confidence in assessing the evolution of the AMOC beyond the 21st century because of the limited number of analyses and equivocal results. However, a collapse beyond the 21st century for large sustained warming cannot be excluded. {12.5} E.5 Cryosphere 17 PM2.5 refers to particulate matter with a diameter of less than 2.5 micrometres, a measure of atmospheric aerosol concentration. 18 The ranges in this paragraph indicate a CMIP5 model spread. â€¢ Year-round reductions in Arctic sea ice extent are projected by the end of the 21st century from multi-model averages. These reductions range from 43% for RCP2.6 to 94% for RCP8.5 in September and from 8% for RCP2.6 to 34% for RCP8.5 in February ( medium confidence ) (see Figures SPM.7 and SPM.8). {12.4}SPM Summary for Policymakers25â€¢ Based on an assessment of the subset of models that most closely reproduce the climatological mean state and 1979 to 2012 trend of the Arctic sea ice extent, a nearly ice-free Arctic Ocean19 in September before mid-century is likely for RCP8.5 (medium confidence ) (see Figures SPM.7 and SPM.8). A projection of when the Arctic might become nearly icefree in September in the 21st century cannot be made with confidence for the other scenarios. {11.3, 12.4, 12.5} â€¢ In the Antarctic, a decrease in sea ice extent and volume is projected with low confidence for the end of the 21st century as global mean surface temperature rises. {12.4} â€¢ By the end of the 21st century, the global glacier volume, excluding glaciers on the periphery of Antarctica, is projected to decrease by 15 to 55% for RCP2.6, and by 35 to 85% for RCP8.5 ( medium\n",
      "\n",
      "\n",
      "\n",
      "Question: What is an ice creame\n",
      "\n",
      "Answer: \n",
      "\n",
      "Using model: llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "# Ensure model is pulled\n",
    "print(f\"Ensuring Ollama model {OLLAMA_MODEL_NAME} is available...\")\n",
    "try:\n",
    "    ollama.pull(OLLAMA_MODEL_NAME)\n",
    "    print(f\"Model {OLLAMA_MODEL_NAME} is ready.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Error pulling model: {e.error}\")\n",
    "    print(\"You might need to install the model manually with: ollama pull \" + OLLAMA_MODEL_NAME)\n",
    "\n",
    "# Get prompt and model\n",
    "prompt, model_name = generate_response_with_ollama(query, results)\n",
    "print('\\n\\n\\n')\n",
    "print(prompt)\n",
    "print(f\"\\nUsing model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt created with 9245 characters\n",
      "First 200 characters of prompt:\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "). These ranges are derived from CMIP5 cli...\n"
     ]
    }
   ],
   "source": [
    "# Print prompt length\n",
    "print(f\"\\nPrompt created with {len(prompt)} characters\")\n",
    "print(\"First 200 characters of prompt:\")\n",
    "print(prompt[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response with Ollama...\n",
      "\n",
      "Response:\n",
      "I'm not able to provide a specific definition for \"ice cream\" as it can refer to different things depending on the context. However, I can provide some possible answers based on common understandings of ice cream:\n",
      "\n",
      "1. In its most basic sense, ice cream is a frozen dessert made from cream, sugar, and flavorings, typically stored in an ice box or freezer.\n",
      "2. In a broader cultural context, ice cream can refer to a sweet treat that originated in the Middle East, where it was first mentioned in ancient times.\n",
      "3. In some regions, particularly in North America, \"ice cream\" might also be used as a slang term for marijuana.\n",
      "\n",
      "If you could provide more context or clarify which aspect of \"ice cream\" you are referring to, I would be more than happy to try and provide a more accurate answer.\n",
      "\n",
      "Response generation complete!\n",
      "Generated 788 characters\n"
     ]
    }
   ],
   "source": [
    "# Give promt as input to ollama generate function\n",
    "\n",
    "# Generate streaming response in Ollama: Setting stream=True when calling ollama.generate(...) \n",
    "# tells the Ollama client to return an iterator/stream of partial response chunks instead of waiting for the full answer. \n",
    "\n",
    "print(\"\\nGenerating response with Ollama...\")\n",
    "response = \"\"\n",
    "print(\"\\nResponse:\")\n",
    "for chunk in ollama.generate(model=model_name, prompt=prompt, stream=True):\n",
    "    piece = chunk['response']\n",
    "    print(piece, end='', flush=True)\n",
    "    response += piece\n",
    "\n",
    "print(\"\\n\\nResponse generation complete!\")\n",
    "print(f\"Generated {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. Connected to OpenSearch and verified the hybrid search pipeline\n",
    "2. Generated embeddings for a search query\n",
    "3. Performed hybrid search combining BM25 and semantic search\n",
    "4. Generated a response using Ollama based on the retrieved documents\n",
    "\n",
    "You can experiment with different:\n",
    "- Search queries\n",
    "- Hybrid search parameters and weights\n",
    "- Ollama models and prompts\n",
    "- Result formatting and processing\n",
    "\n",
    "This completes the basic RAG (Retrieval Augmented Generation) pipeline that combines the power of OpenSearch for hybrid retrieval with the generation capabilities of LLMs through Ollama."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
