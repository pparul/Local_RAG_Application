{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# OpenSearch Index Creation and Document Ingestion\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Create an OpenSearch index with proper mappings for hybrid search\n",
        "2. Process PDF documents into chunks\n",
        "3. Generate embeddings for the chunks\n",
        "4. Ingest the chunks with their embeddings into OpenSearch\n",
        "\n",
        "All functions are defined directly in this notebook, allowing you to modify them and experiment with different approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/parulpandey/Library/CloudStorage/OneDrive-Personal/Ext Github Repos/RAG_UI_fresh/.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from opensearchpy import OpenSearch, helpers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Python path to access project modules\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constants defined. You can modify these values to experiment with different settings.\n"
          ]
        }
      ],
      "source": [
        "# Define constants\n",
        "# You can modify these values to experiment with different settings\n",
        "\n",
        "# OpenSearch connection settings\n",
        "OPENSEARCH_HOST = \"localhost\"  # OpenSearch host\n",
        "OPENSEARCH_PORT = 9200  # OpenSearch port\n",
        "OPENSEARCH_INDEX = \"tech-document-2\"  # Index name for document storage\n",
        "\n",
        "# Embedding settings\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for generating embeddings\n",
        "EMBEDDING_DIMENSION = 384  # Embedding dimension for the model\n",
        "ASSYMETRIC_EMBEDDING = False  # Whether to use asymmetric embeddings\n",
        "\n",
        "# Chunking settings\n",
        "TEXT_CHUNK_SIZE = 500  # Number of tokens per chunk\n",
        "TEXT_CHUNK_OVERLAP = 100  # Overlap between chunks\n",
        "\n",
        "print(\"Constants defined. You can modify these values to experiment with different settings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility functions defined. You can modify these functions to experiment with different text processing techniques.\n"
          ]
        }
      ],
      "source": [
        "# Utility functions for text processing\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans OCR-extracted text by removing unnecessary newlines, hyphens, and correcting common OCR errors.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    # Remove hyphens at line breaks (e.g., 'exam-\\nple' -> 'example')\n",
        "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
        "\n",
        "    # Replace newlines within sentences with spaces\n",
        "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
        "\n",
        "    # Replace multiple newlines with a single newline\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits text into chunks with a specified overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to split.\n",
        "        chunk_size (int): The number of tokens in each chunk.\n",
        "        overlap (int): The number of tokens to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    # Clean the text before chunking\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = text.split(\" \")\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens): # PP@ this is an infinite loop if wrong inputs are given\n",
        "        end = start + chunk_size\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk_text = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        start = end - overlap  # Move back by 'overlap' tokens\n",
        "\n",
        "    return chunks\n",
        "\n",
        "print(\"Utility functions defined. You can modify these functions to experiment with different text processing techniques.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = \"wow what a great day . could become shitty at any point. but for now, it is a great day\"\n",
        "# TEXT_CHUNK_OVERLAP=2\n",
        "# TEXT_CHUNK_SIZE = 5\n",
        "# chunks = chunk_text(text, TEXT_CHUNK_SIZE, TEXT_CHUNK_OVERLAP)\n",
        "# chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding functions defined. You can modify these functions to experiment with different embedding techniques.\n"
          ]
        }
      ],
      "source": [
        "# Embedding functions\n",
        "\n",
        "def get_embedding_model():\n",
        "    \"\"\"\n",
        "    Loads and returns the sentence transformer embedding model.\n",
        "    \n",
        "    Returns:\n",
        "        SentenceTransformer: The loaded embedding model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_embeddings(texts: List[str]):\n",
        "    \"\"\"\n",
        "    Generates embeddings for a list of text chunks.\n",
        "    \n",
        "    Args:\n",
        "        texts (List[str]): List of text chunks to embed.\n",
        "        \n",
        "    Returns:\n",
        "        List[numpy.ndarray]: List of embedding vectors.\n",
        "    \"\"\"\n",
        "    model = get_embedding_model()\n",
        "    \n",
        "    # If using asymmetric embeddings, prefix each text with \"passage: \"\n",
        "    if ASSYMETRIC_EMBEDDING:\n",
        "        texts = [f\"passage: {text}\" for text in texts]\n",
        "        \n",
        "    # Generate embeddings\n",
        "    embeddings = model.encode(texts)\n",
        "    return embeddings\n",
        "\n",
        "print(\"Embedding functions defined. You can modify these functions to experiment with different embedding techniques.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embeddings =  generate_embeddings(chunks)\n",
        "# print(f\"Generated {len(chunks)} chunks\")\n",
        "# print(f\"Generated {len(embeddings)} embeddings\")\n",
        "# print(f\"Embedding shape: {embeddings[0].shape}\")\n",
        "# embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Connect to OpenSearch and Create Index\n",
        "\n",
        "Now that we have all our utility functions defined, let's connect to OpenSearch and create an index with the right mappings for hybrid search.\n",
        "\n",
        "The index configuration includes three main components:\n",
        "1. **Text Field (`text`)**: Used for full-text search with BM25 algorithm\n",
        "2. **Vector Field (`embedding`)**: Used for semantic search using KNN\n",
        "3. **Metadata Field (`document_name`)**: Used for filtering and organizing documents\n",
        "\n",
        "Make sure you have OpenSearch running locally (typically in a Docker container).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully connected to OpenSearch 2.11.0\n"
          ]
        }
      ],
      "source": [
        "# Create an OpenSearch client\n",
        "client = OpenSearch(\n",
        "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
        "    http_compress=True,\n",
        "    timeout=30,\n",
        "    max_retries=3,\n",
        "    retry_on_timeout=True,\n",
        ")\n",
        "\n",
        "# Check connection\n",
        "try:\n",
        "    info = client.info()\n",
        "    print(f\"Successfully connected to OpenSearch {info['version']['number']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to OpenSearch: {e}\")\n",
        "    print(\"Make sure OpenSearch is running on localhost:9200\")\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Index Configuration:\n",
            "{\n",
            "  \"settings\": {\n",
            "    \"index\": {\n",
            "      \"number_of_shards\": 1,\n",
            "      \"number_of_replicas\": 0,\n",
            "      \"knn\": true\n",
            "    }\n",
            "  },\n",
            "  \"mappings\": {\n",
            "    \"properties\": {\n",
            "      \"text\": {\n",
            "        \"type\": \"text\"\n",
            "      },\n",
            "      \"embedding\": {\n",
            "        \"type\": \"knn_vector\",\n",
            "        \"dimension\": 384,\n",
            "        \"method\": {\n",
            "          \"engine\": \"faiss\",\n",
            "          \"space_type\": \"l2\",\n",
            "          \"name\": \"hnsw\",\n",
            "          \"parameters\": {}\n",
            "        }\n",
            "      },\n",
            "      \"document_name\": {\n",
            "        \"type\": \"keyword\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Define the index configuration\n",
        "def create_index_config() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Creates the index configuration with mappings for text, embeddings, and metadata.\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, Any]: The index configuration.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"settings\": {\n",
        "            \"index\": {\n",
        "                \"number_of_shards\": 1,\n",
        "                \"number_of_replicas\": 0,\n",
        "                \"knn\": True\n",
        "            }\n",
        "        },\n",
        "        \"mappings\": {\n",
        "            \"properties\": {\n",
        "                \"text\": {\n",
        "                    \"type\": \"text\"  # For standard text search\n",
        "                },\n",
        "                \"embedding\": {\n",
        "                    \"type\": \"knn_vector\",\n",
        "                    \"dimension\": EMBEDDING_DIMENSION,  # Match your embedding model's dimension\n",
        "                    \"method\": {\n",
        "                        \"engine\": \"faiss\",\n",
        "                        \"space_type\": \"l2\",\n",
        "                        \"name\": \"hnsw\",\n",
        "                        \"parameters\": {}\n",
        "                    }\n",
        "                },\n",
        "                \"document_name\": {\n",
        "                    \"type\": \"keyword\"  # For exact match on document names\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return config\n",
        "\n",
        "# Get the index configuration\n",
        "index_config = create_index_config()\n",
        "print(\"\\nIndex Configuration:\")\n",
        "print(json.dumps(index_config, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Created index tech-document-2 with response: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'tech-document-2'}\n"
          ]
        }
      ],
      "source": [
        "# Create the index if it doesn't exist\n",
        "if not client.indices.exists(index=OPENSEARCH_INDEX):\n",
        "    response = client.indices.create(index=OPENSEARCH_INDEX, body=index_config)\n",
        "    print(f\"\\nCreated index {OPENSEARCH_INDEX} with response: {response}\")\n",
        "else:\n",
        "    print(f\"\\nIndex {OPENSEARCH_INDEX} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Use script #1 to set up the pipeline using Opensearch Dashboard at port 5601"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Search pipeline 'personal-paper-search-pipeline' exists.\n",
            "{'personal-paper-search-pipeline': {'description': 'Post processor for hybrid search', 'phase_results_processors': [{'normalization-processor': {'normalization': {'technique': 'min_max'}, 'combination': {'technique': 'arithmetic_mean', 'parameters': {'weights': [0.3, 0.7]}}}}]}}\n"
          ]
        }
      ],
      "source": [
        "from opensearchpy.exceptions import NotFoundError\n",
        "#pipeline_name = \"nlp-search-pipeline\"\n",
        "pipeline_name = \"personal-paper-search-pipeline\"\n",
        "\n",
        "try:\n",
        "    result = client.transport.perform_request(\n",
        "        \"GET\",\n",
        "        f\"/_search/pipeline/{pipeline_name}\"\n",
        "    )\n",
        "    print(f\"\\n‚úÖ Search pipeline '{pipeline_name}' exists.\")\n",
        "    print(result)\n",
        "except NotFoundError:\n",
        "    print(f\"\\n‚ö†Ô∏è Search pipeline '{pipeline_name}' does NOT exist.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nüö® Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Process PDF Document\n",
        "\n",
        "Now let's process a PDF document to extract its content:\n",
        "1. Read the PDF and extract the text\n",
        "2. Clean and chunk the text into smaller segments\n",
        "3. Generate embeddings for each chunk\n",
        "\n",
        "You can replace the PDF path with your own document if you want to experiment with different content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/parulpandey/Library/CloudStorage/OneDrive-Personal/Ext Github Repos/RAG_UI_fresh/notebooks\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 46610 characters from Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning.pdf\n",
            "\n",
            "Sample of extracted text:\n",
            "Citation: Kim, Y.J.; Song, J.H.; Cho,\n",
            "K.H.; Shin, J.H.; Kim, J.S.; Yoon, J.S.;\n",
            "Hong, S.J. Improved Plasma Etch\n",
            "Endpoint Detection Using\n",
            "Attention-Based Long Short-Term\n",
            "Memory Machine Learning.\n",
            "Electronics 2024 ,13, 3577. https://\n",
            "doi.org/10.3390/electronics13173577\n",
            "Academic Editors: Claudio Turchetti\n",
            "and Laura Falaschetti\n",
            "Received: 31 July 2024\n",
            "Revised: 2 September 2024\n",
            "Accepted: 5 September 2024\n",
            "Published: 9 September 2024\n",
            "Copyright: ¬©2024 by the authors.\n",
            "Licensee MDPI, Basel, Switzerland.\n",
            "This...\n",
            "\n",
            "Text cleaned. Length: 46528 characters\n",
            "Split text into 15 chunks\n",
            "\n",
            "Sample chunk:\n",
            "Citation: Kim, Y.J.; Song, J.H.; Cho, K.H.; Shin, J.H.; Kim, J.S.; Yoon, J.S.; Hong, S.J. Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learning. Electronics 2024 ,13, 3577. https:// doi.org/10.3390/electronics13173577 Academic Editors: Claudio Turchetti and Laura Falaschetti Received: 31 July 2024 Revised: 2 September 2024 Accepted: 5 September 2024 Published: 9 September 2024 Copyright: ¬©2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). electronics Article Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learning Ye Jin Kim1, Jung Ho Song2, Ki Hwan Cho2, Jong Hyeon Shin2, Jong Sik Kim2 , Jung Sik Yoon2 and Sang Jeen Hong1,* 1Department of Semiconductor Engineering, Myongji University, Yongin 17058, Republic of Korea; 80231133@mju.ac.kr 2Plasma E.I. Convergence Research Center, Korea Research Institute of Fusion Energy, Daejeon 34133, Republic of Korea; jungho@kfe.re.kr (J.H.S.); khcho@kfe.re.kr (K.H.C.); shin0020@kfe.re.kr (J.H.S.); jongsik@kfe.re.kr (J.S.K.); jsyoon@kfe.re.kr (J.S.Y.) *Correspondence: samhong@mju.ac.kr Abstract: Existing etch endpoint detection (EPD) methods, primarily based on single wavelengths, have limitations, such as low signal-to-noise ratios and the inability to consider the long-term dependencies of time series data. To address these issues, this study proposes a context of time series data using long short-term memory (LSTM), a kind of recurrent neural network (RNN). The proposed method is based on the time series data collected through optical emission spectroscopy (OES) data during the SiO 2etching process. After training the LSTM model, the proposed method demonstrated the ability to detect the etch endpoint more accurately than existing methods by considering the entire time series. The LSTM model achieved an accuracy of 97.1% in a given condition, which shows that considering the flow and context of time series data can significantly reduce the false detection rate. To improve the performance of the proposed LSTM model, we created an attention-based LSTM model and confirmed that the model accuracy is 98.2%, and the performance is improved compared to that of the existing LSTM model. Keywords: plasma etch; endpoint detection; machine learning 1. Introduction Over the past decade, 3D-NAND flash memory technology has rapidly enhanced the bit storage density per unit area by increasing the number of vertically stacked gate layers. In recent years, the semiconductor industry has witnessed a significant increase in demand for high-capacity storage devices, such as 3D-NAND flash memory, driven by the rapid growth of various sectors, including cloud services and mobile devices. The latest generation of 3D-NAND features more than 200 layers of vertical gate stacks. To manufacture such a highly integrated 3D-NAND flash memory, high-aspect-ratio (HAR) etching technology is essential, enabling the precise etching of numerous layers, ranging in the several hundreds [ 1,2]. In these structures, accurately controlling the etching depth and precisely detecting the etching endpoint for each layer has become increasingly important. Consequently, reliable etch endpoint detection (EPD) techniques are crucial. EPD plays a vital role in determining the yield and quality of the etching process, and its\n",
            "shape of chunks: 15\n"
          ]
        }
      ],
      "source": [
        "# Read and process the PDF\n",
        "pdf_path = \"attention is all you need.pdf\"  # Path relative to notebook directory\n",
        "pdf_path = \"Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning.pdf\"\n",
        "\n",
        "# Read the PDF file\n",
        "\n",
        "reader = PdfReader(pdf_path)\n",
        "text = \"\".join([page.extract_text() for page in reader.pages])\n",
        "print(f\"Extracted {len(text)} characters from {pdf_path}\")\n",
        "\n",
        "# Show a sample of the extracted text\n",
        "print(\"\\nSample of extracted text:\")\n",
        "print(text[:500] + \"...\")\n",
        "\n",
        "# Clean the text\n",
        "cleaned_text = clean_text(text)\n",
        "print(f\"\\nText cleaned. Length: {len(cleaned_text)} characters\")\n",
        "\n",
        "# Chunk the text\n",
        "chunks = chunk_text(cleaned_text, chunk_size=TEXT_CHUNK_SIZE, overlap=TEXT_CHUNK_OVERLAP)\n",
        "print(f\"Split text into {len(chunks)} chunks\")\n",
        "\n",
        "# Display a sample chunk\n",
        "print(\"\\nSample chunk:\")\n",
        "print(chunks[0])\n",
        "print(f'shape of chunks: {len(chunks)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for chunks. This might take a moment...\n",
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Generated 15 chunks\n",
            "Generated 15 embeddings\n",
            "Embedding shape: (384,)\n",
            "Embedding shape: (15, 384)\n",
            "\n",
            "Sample embedding:\n",
            "[ 1.05578750e-02  6.27988353e-02  7.31234998e-02 -1.91180296e-02\n",
            "  6.34288266e-02  2.84780236e-03 -2.56312601e-02 -3.32025476e-02\n",
            " -3.85175401e-04 -4.29034270e-02 -3.03010773e-02 -4.74573635e-02\n",
            " -3.76687311e-02 -4.51399712e-04 -1.03420904e-02 -6.69936612e-02\n",
            "  3.93932946e-02 -2.49275193e-02  1.27381766e-02 -2.90004369e-02\n",
            "  6.56380951e-02 -2.35572457e-02  2.47118203e-03  1.78134406e-03\n",
            " -3.48995142e-02  6.87616244e-02  6.14231415e-02  3.15121487e-02\n",
            " -5.21094389e-02 -4.79027852e-02  2.80021615e-02 -2.75245998e-02\n",
            " -3.21146883e-02  4.95025851e-02 -5.21548502e-02  1.91652849e-02\n",
            " -5.89792356e-02  5.31596802e-02 -4.61701043e-02  9.97116556e-04\n",
            "  3.27665098e-02 -7.22514391e-02 -2.38627102e-02 -1.45818666e-02\n",
            "  1.49840891e-01  7.41566624e-03  2.69192588e-02 -1.56620964e-01\n",
            "  3.35796960e-02 -8.26908625e-04 -1.51355015e-02  2.86633265e-03\n",
            "  2.29143202e-02  3.86241823e-02 -1.33289152e-03  1.85613055e-02\n",
            "  3.52337807e-02  2.78473496e-02 -4.16376069e-02  2.77161505e-02\n",
            " -2.08066329e-02 -1.07929595e-01  1.05201351e-02  7.32992077e-03\n",
            " -1.31372577e-02 -7.47290626e-02  1.01477824e-01  2.27355622e-02\n",
            " -8.51105750e-02 -6.95540085e-02 -2.51849424e-02  9.62255150e-02\n",
            "  2.53460724e-02  7.83252157e-03  3.09957117e-02  6.97776442e-03\n",
            " -4.85820323e-02 -4.17952947e-02  1.32660791e-02 -6.03086315e-02\n",
            " -2.26632897e-02 -1.49908029e-02 -1.91608537e-02  3.91573720e-02\n",
            "  3.77997234e-02  7.00156316e-02 -6.28344715e-03 -9.92856547e-03\n",
            " -7.61344135e-02 -3.28056812e-02  1.04363253e-02 -1.56729281e-01\n",
            "  9.42604803e-03 -4.15135138e-02  9.11009964e-03  7.54208211e-03\n",
            "  3.92225869e-02 -3.73900458e-02 -5.43686189e-02  4.96192323e-03\n",
            "  1.58746075e-02  4.02431935e-02 -3.86010073e-02 -2.46546660e-02\n",
            "  6.57328740e-02  7.04056537e-03  3.54513004e-02  8.79594460e-02\n",
            " -1.26315486e-02 -5.18970098e-03  4.60265111e-03  3.81708741e-02\n",
            " -4.18951958e-02 -1.81345008e-02  1.04272082e-01 -4.82448190e-03\n",
            " -2.94824019e-02 -6.22903444e-02 -2.73588859e-02  9.94324125e-03\n",
            " -2.24132296e-02 -1.08129837e-01 -9.94903520e-02  1.67627260e-01\n",
            "  2.50708498e-02 -4.39791335e-03  1.66838337e-02  7.20705578e-33\n",
            " -2.15413254e-02 -2.85362266e-02  2.48511247e-02 -5.95638305e-02\n",
            " -4.21877392e-02  5.26394993e-02 -1.43880174e-02  4.87910695e-02\n",
            "  3.53346608e-04  4.49043103e-02  9.24242847e-03  4.11139913e-02\n",
            " -2.90891267e-02  7.36808106e-02  4.21420187e-02 -1.15539096e-01\n",
            "  1.18802050e-02  1.23877442e-02 -6.71999604e-02  5.01014898e-03\n",
            "  7.84589127e-02 -5.21563701e-02  3.65839116e-02 -4.51900549e-02\n",
            "  8.88567641e-02  8.27925280e-02 -5.88489585e-02  6.59417063e-02\n",
            " -5.11546209e-02  7.27991853e-03  6.75009424e-03  2.45108716e-02\n",
            " -2.30244193e-02 -1.96353421e-02 -5.90694770e-02 -9.16233063e-02\n",
            " -5.86962886e-02  2.59957127e-02  7.54412115e-02 -6.67473525e-02\n",
            " -3.96964662e-02  4.97610308e-03 -7.97191113e-02 -2.40945611e-02\n",
            " -4.28621508e-02 -2.51945714e-03  1.49920760e-02  4.16844077e-02\n",
            " -3.30452509e-02  1.90901514e-02 -1.13841258e-02 -2.44673379e-02\n",
            " -4.58345674e-02 -2.01056600e-02  4.89293821e-02  9.72581431e-02\n",
            "  9.14617479e-02  1.81647781e-02 -1.34939060e-03  5.96220307e-02\n",
            "  2.89070923e-02  7.43743777e-02 -6.20991066e-02  6.75658509e-02\n",
            " -2.74978913e-02  2.47926041e-02  5.61680906e-02  8.36997852e-02\n",
            " -1.22067016e-02 -1.38291772e-02  4.19262610e-02 -2.19703000e-02\n",
            "  4.51164097e-02  2.04639360e-02  2.70793829e-02 -7.87667558e-02\n",
            "  2.17792150e-02  9.33516696e-02  7.96499625e-02 -1.12202637e-01\n",
            "  4.94141243e-02 -9.52104703e-02 -2.96193780e-03 -2.42324267e-02\n",
            " -2.81993840e-02 -4.98156995e-02 -1.77906752e-02 -1.26360849e-01\n",
            " -1.65552214e-01 -6.08574338e-02 -2.14564353e-02  3.51705849e-02\n",
            "  5.92922866e-02  5.83199784e-02 -1.66569963e-01 -8.87798206e-33\n",
            "  4.86318916e-02  1.02663487e-02  4.93950723e-03  5.09276427e-03\n",
            " -5.27088623e-03  3.95306386e-02 -5.56489080e-03  7.01795295e-02\n",
            " -4.90256920e-02  5.15312515e-02  1.37213047e-03  1.66336726e-02\n",
            "  8.99385288e-03 -6.02998585e-02  3.38490754e-02  6.02884665e-02\n",
            " -6.29268214e-03  1.53930169e-02  4.51401100e-02 -3.77745256e-02\n",
            "  2.70622279e-02  1.85518321e-02  3.82078402e-02 -3.49057987e-02\n",
            " -3.50957327e-02  4.33117040e-02  3.03731160e-03  1.51634058e-02\n",
            " -5.73137216e-02 -2.50727069e-02 -4.71090414e-02  3.50035913e-02\n",
            " -1.26809925e-02  1.49497427e-02  4.44825403e-02 -2.71711405e-02\n",
            "  1.06125884e-02  4.20809630e-03  1.94927864e-02 -3.51126082e-02\n",
            "  8.55658427e-02  1.56338066e-02 -7.22899809e-02  1.51441246e-03\n",
            " -3.91204059e-02  6.16887957e-02 -6.96128756e-02  8.14065039e-02\n",
            "  6.46315292e-02 -4.63563800e-02 -7.17316568e-02 -7.66584883e-03\n",
            " -6.55093119e-02  2.98957489e-02 -9.66105536e-02  6.97263926e-02\n",
            "  1.30328415e-02  3.74137498e-02  2.81599108e-02 -9.76302549e-02\n",
            "  3.24849389e-03 -8.12673196e-02  3.11408956e-02  3.23127233e-03\n",
            " -9.75385029e-03 -6.14388548e-02  5.40187582e-02  1.32335173e-02\n",
            "  5.45639098e-02 -2.65134033e-02  2.75978558e-02  1.42920325e-02\n",
            "  2.86468063e-02 -4.16283831e-02 -1.28431261e-01  1.31783402e-02\n",
            " -8.49196166e-02 -1.03746697e-01 -2.20299289e-02 -3.41046639e-02\n",
            "  1.73051357e-02 -2.80519705e-02 -7.36411810e-02  8.12390745e-02\n",
            "  5.64111769e-02  4.04264145e-02  1.01918681e-02 -1.86913759e-02\n",
            "  2.86104605e-02 -7.69886887e-03 -1.42068174e-02  1.00183286e-01\n",
            " -1.31805567e-02  2.75659282e-02 -3.13877724e-02 -5.86404703e-08\n",
            " -2.06997637e-02 -6.58048540e-02  5.72691225e-02  4.44178935e-03\n",
            "  1.72362588e-02 -8.04148689e-02  6.70015905e-03  4.38344255e-02\n",
            "  2.31022062e-03 -5.24508134e-02  8.06671381e-02  2.84820260e-03\n",
            " -5.63311018e-03  5.22783771e-02  5.13196029e-02  5.49345315e-02\n",
            "  5.41364960e-02 -3.06604966e-03 -7.67885661e-03  3.48775461e-02\n",
            " -1.49751939e-02 -1.41348839e-02  5.30630350e-02 -2.95748450e-02\n",
            "  3.07200365e-02  4.85756574e-03 -1.61752254e-02  5.91000840e-02\n",
            "  4.16788571e-02 -9.43294168e-02 -3.87878083e-02 -9.81029775e-03\n",
            "  1.89918224e-02  6.09446168e-02  5.92420809e-02  8.77666622e-02\n",
            "  4.65557314e-02  2.69636661e-02  1.15519436e-03  5.35050929e-02\n",
            " -7.34166130e-02 -4.63746972e-02 -2.87032388e-02  3.20938826e-02\n",
            " -4.21486050e-03 -3.62872449e-03  7.23779723e-02 -6.40406907e-02\n",
            "  1.93863101e-02 -5.50413243e-02  8.94739777e-02 -2.79436763e-02\n",
            "  1.42927691e-02 -9.58177671e-02 -2.50629913e-02  1.54629340e-02\n",
            " -5.43955453e-02 -3.87197621e-02  5.34293540e-02  6.58964589e-02\n",
            "  9.56278220e-02 -2.83128954e-02 -7.51577318e-02  2.79851947e-02]\n",
            "\n",
            "Prepared 15 documents for indexing\n"
          ]
        }
      ],
      "source": [
        "pdf_file_name = pdf_path.replace('.pdf', '')\n",
        "\n",
        "# Generate embeddings for the chunks\n",
        "print(\"Generating embeddings for chunks. This might take a moment...\")\n",
        "embeddings = generate_embeddings(chunks)\n",
        "print(f\"Generated {len(chunks)} chunks\")\n",
        "print(f\"Generated {len(embeddings)} embeddings\")\n",
        "print(f\"Embedding shape: {embeddings[0].shape}\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "# Display a sample embedding (just a few values to avoid clutter)\n",
        "print(\"\\nSample embedding:\")\n",
        "print(embeddings[0])\n",
        "\n",
        "# Prepare documents for indexing\n",
        "documents_to_index = [\n",
        "    {\n",
        "        \"doc_id\": f\"{pdf_file_name}_{i}\",\n",
        "        \"text\": chunk,\n",
        "        \"embedding\": embedding,\n",
        "        \"document_name\": pdf_file_name,\n",
        "    }\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
        "]\n",
        "\n",
        "print(f\"\\nPrepared {len(documents_to_index)} documents for indexing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'doc_id': 'Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning_1',\n",
              " 'text': 'growth of various sectors, including cloud services and mobile devices. The latest generation of 3D-NAND features more than 200 layers of vertical gate stacks. To manufacture such a highly integrated 3D-NAND flash memory, high-aspect-ratio (HAR) etching technology is essential, enabling the precise etching of numerous layers, ranging in the several hundreds [ 1,2]. In these structures, accurately controlling the etching depth and precisely detecting the etching endpoint for each layer has become increasingly important. Consequently, reliable etch endpoint detection (EPD) techniques are crucial. EPD plays a vital role in determining the yield and quality of the etching process, and its significance continues to grow [3]. Accurate etch endpoint detection plays a crucial role in improving manufacturing process yield and reducing process failure rates. In the plasma etching process, the intensity of the wavelength corresponding to the thin-film layer to be etched is monitored, and the change in intensity provides a real-time view of the etching progress. The principle of EPD is that when the etched material is fully reacted, and no more by-products are produced, a change in the intensity of the corresponding wavelengths is detected. Various EPD techniques have been developed in recent years [ 4,5]. However, several limitations Electronics 2024 ,13, 3577. https://doi.org/10.3390/electronics13173577 https://www.mdpi.com/journal/electronicsElectronics 2024 ,13, 3577 2 of 11 still exist. Traditional EPD algorithms have primarily relied on signal changes at a single wavelength [ 6‚Äì8]. Such single-wavelength-based EPD methods are vulnerable to noise and signal fluctuations, which can lead to a low signal-to-noise ratio [ 9,10]. This may pose a challenge to the precision of endpoint detection. While recent studies have focused on improving EPD performance by employing multiple wavelengths [ 11,12], most EPD algorithms still rely on signal patterns at specific time points for decision-making. This reliance on instantaneous signal patterns makes it challenging to predict the EPD moment in advance or accurately detect the endpoint, as the algorithms mainly focus on the signal change at the exact EPD time. On the other hand, the optical emission spectroscopy (OES) time series data measured during the entire etching process contains information spanning from the beginning to the end of the etching procedure. However, existing EPD algorithms often struggle to fully account for the temporal dependencies embedded within these time series data, lacking consideration for the comprehensive flow and context of the data. As a result, there is a pressing need for data analysis and decision-making approaches that effectively incorporate these long-term temporal dependencies. In recent years, efforts have been made to address these issues by leveraging deep learning techniques [ 13‚Äì16]. Kim et al. utilized a convolutional neural network (CNN) based on OES data to detect the etch endpoint [ 17], but it has a limitation in directly modeling the temporal dependencies of time series data, as the authors transformed the OES data into a two-dimensional format to be used as input for the CNN. Hwang et al. proposed a method for detecting anomalies in time series data from semiconductor manufacturing processes using a long short-term',\n",
              " 'embedding': array([-6.41053990e-02,  4.70145568e-02,  3.76167186e-02, -9.48947296e-02,\n",
              "         3.66584770e-02, -2.92829536e-02, -3.91566157e-02, -2.48285141e-02,\n",
              "         3.75471115e-02, -2.51754224e-02,  2.77917143e-02, -8.42251349e-03,\n",
              "        -5.98664992e-02,  3.73306423e-02,  2.62685888e-03, -1.08370399e-02,\n",
              "         1.28813600e-02,  1.46984961e-02,  1.49401529e-02, -3.20434831e-02,\n",
              "         7.75706470e-02, -4.63487431e-02,  3.93163273e-03,  1.97003316e-02,\n",
              "        -7.57128522e-02,  1.41823515e-02,  7.16791600e-02, -2.94792801e-02,\n",
              "         8.81971139e-03, -7.16933906e-02,  4.01806161e-02, -4.64165024e-03,\n",
              "        -1.14716776e-01,  3.15074697e-02, -1.79187953e-02, -2.19341088e-02,\n",
              "         7.13900179e-02, -1.09665561e-03, -7.35656470e-02, -4.72854264e-02,\n",
              "         1.29770627e-02, -2.50737015e-02,  3.97082930e-03, -2.54557058e-02,\n",
              "         8.38706195e-02,  2.08030269e-02,  2.77549252e-02, -7.99438879e-02,\n",
              "        -2.38431006e-04, -8.57711509e-02,  9.65911821e-02,  2.87041347e-02,\n",
              "         5.60810007e-02, -5.38011044e-02,  3.59669439e-02,  7.68749043e-02,\n",
              "         1.72663592e-02, -2.38097236e-02, -4.60444726e-02,  6.92885444e-02,\n",
              "         1.99343059e-02, -3.23098712e-02,  1.26101542e-02, -1.89455282e-02,\n",
              "         1.95257515e-02, -7.52381608e-03,  4.56702113e-02, -9.19009745e-02,\n",
              "        -4.04436216e-02, -3.35296728e-02,  1.44723495e-02,  1.15409503e-02,\n",
              "         3.48368520e-03,  4.96110581e-02,  1.58236157e-02,  6.11698814e-03,\n",
              "        -1.57811623e-02, -6.13159209e-04, -3.06520574e-02, -7.56132901e-02,\n",
              "         2.72080544e-02,  2.66347104e-03,  4.01751548e-02,  4.45233062e-02,\n",
              "        -1.68629698e-02,  2.82118674e-02, -1.01570645e-03,  3.34385410e-02,\n",
              "        -2.83236820e-02,  1.60416123e-02, -3.50577496e-02, -5.33865206e-02,\n",
              "        -1.06293544e-01,  4.88519259e-02,  3.95389162e-02,  8.46803561e-03,\n",
              "         3.37608047e-02, -5.19713759e-02,  1.72707681e-02, -5.09460308e-02,\n",
              "         1.25394845e-02,  9.99614783e-03, -4.16617431e-02, -2.16120761e-02,\n",
              "         6.66887080e-03,  6.14235178e-02,  2.59478739e-03,  9.77946818e-02,\n",
              "        -1.40899532e-02,  2.90752500e-02,  1.07526360e-02, -4.37536836e-03,\n",
              "        -1.05075268e-02,  2.92297173e-02,  5.81697226e-02, -7.60940015e-02,\n",
              "        -4.18335982e-02, -4.37570140e-02, -3.19760777e-02, -5.44182882e-02,\n",
              "        -4.70509902e-02, -2.97032446e-02, -5.42990752e-02,  1.04692817e-01,\n",
              "        -3.41432989e-02,  3.00750565e-02,  6.68703765e-02,  2.08171895e-33,\n",
              "         3.90331331e-03,  9.50509775e-03, -7.19213113e-02, -7.44045004e-02,\n",
              "        -8.07385892e-02,  8.91773254e-02,  5.71008958e-02,  2.66368315e-02,\n",
              "         2.58887075e-02, -1.75350718e-02,  6.18324755e-03, -2.11256593e-02,\n",
              "        -7.08575398e-02,  5.38172498e-02,  8.60043168e-02, -9.53094810e-02,\n",
              "         5.69297597e-02,  1.93951987e-02, -5.60791865e-02, -3.92787624e-03,\n",
              "         1.07631749e-02, -6.49430305e-02,  1.23145962e-02, -2.51739975e-02,\n",
              "         1.00287363e-01,  1.41823394e-02, -3.69869247e-02,  2.66452748e-02,\n",
              "        -3.85847092e-02, -2.13697832e-02,  1.08863473e-01, -8.74532014e-03,\n",
              "         3.16846110e-02, -1.01404570e-01, -1.43311555e-02,  1.92885511e-02,\n",
              "        -4.65075001e-02,  5.40753528e-02,  4.36705463e-02, -3.78006510e-02,\n",
              "        -3.45555018e-03,  3.35593224e-02, -2.38489918e-02, -2.24707201e-02,\n",
              "        -7.03445673e-02, -1.38244899e-02, -1.58730894e-02,  5.36166839e-02,\n",
              "        -7.67706931e-02,  2.40155477e-02, -2.70843133e-02,  9.75908712e-03,\n",
              "         9.88315791e-03, -5.41300746e-03,  3.78641896e-02,  3.47019210e-02,\n",
              "         1.41658848e-02, -5.83955050e-02,  3.65863740e-02,  6.04195632e-02,\n",
              "        -1.01594441e-02,  3.83690000e-02, -6.00476377e-02,  1.09622203e-01,\n",
              "        -6.84674606e-02, -8.76109023e-03,  8.46571475e-02,  3.96061763e-02,\n",
              "        -5.20216413e-02,  3.95807661e-02, -1.28171751e-02, -2.75607631e-02,\n",
              "         6.62555248e-02, -2.39803642e-02, -2.22689100e-02, -6.63421378e-02,\n",
              "        -1.85924899e-02,  4.20860872e-02,  1.01908483e-01, -5.18604331e-02,\n",
              "         1.47604570e-02, -1.85081214e-02, -1.35386474e-02, -7.34911188e-02,\n",
              "        -3.74545604e-02,  6.85049891e-02, -7.61622004e-03, -3.14393342e-02,\n",
              "        -4.32931744e-02,  1.73671711e-02,  6.26899004e-02, -4.07105237e-02,\n",
              "         1.28689781e-01,  3.03194113e-02, -6.40429854e-02, -2.33246218e-33,\n",
              "         3.92880812e-02, -5.48948757e-02,  4.68462594e-02, -1.10547086e-02,\n",
              "        -1.50541179e-02,  8.65174271e-03,  4.66856286e-02,  1.11527227e-01,\n",
              "         4.98469966e-03, -2.05044318e-02, -2.04831474e-02,  5.33875190e-02,\n",
              "        -6.47771806e-02, -3.09138577e-02, -2.89730784e-02,  5.03943674e-02,\n",
              "        -6.66499957e-02, -3.81094329e-02,  9.91391018e-02, -3.28568891e-02,\n",
              "         5.65144457e-02, -2.75569092e-02, -1.55294768e-03,  2.44451798e-02,\n",
              "         6.01752810e-02,  6.43039793e-02,  2.90622227e-02, -5.51263802e-02,\n",
              "        -3.16880383e-02,  1.62544346e-03,  1.03233913e-02,  1.13135930e-02,\n",
              "         3.25192697e-02,  3.38915586e-02,  5.07607386e-02, -1.02236792e-01,\n",
              "         1.10867299e-01, -2.06994247e-02,  4.95816879e-02, -1.00046955e-01,\n",
              "        -2.20423192e-02,  9.19559151e-02, -9.90739092e-02, -5.72426803e-02,\n",
              "        -3.31093185e-02,  1.09905735e-01, -4.14079875e-02,  8.32790881e-02,\n",
              "         5.90622313e-02, -6.66349679e-02, -9.22932550e-02,  1.62270498e-02,\n",
              "         3.71037535e-02,  7.87855983e-02, -8.27597920e-03,  3.92419705e-03,\n",
              "        -1.49378730e-02, -7.07759149e-03,  2.23132055e-02, -1.17153190e-02,\n",
              "         4.81919311e-02, -4.65420149e-02,  4.59102169e-02, -4.38400395e-02,\n",
              "         2.33460451e-03, -3.47325131e-02,  5.23946062e-02, -4.44267085e-03,\n",
              "        -2.97293533e-02, -6.90661930e-03, -1.49626052e-02, -2.97615360e-02,\n",
              "         5.29988147e-02, -4.89342175e-02, -6.40793666e-02, -3.42102684e-02,\n",
              "        -3.65083516e-02, -1.17119908e-01,  1.25440331e-02,  4.52896915e-02,\n",
              "        -7.71778077e-03, -8.26087687e-03, -5.18265404e-02,  1.05228521e-01,\n",
              "         1.60154421e-02,  7.59478984e-03, -1.71436789e-03,  1.41332373e-02,\n",
              "        -9.78135411e-03,  2.70382259e-02, -1.07314968e-02,  5.06429262e-02,\n",
              "        -1.17794601e-02,  8.38257093e-03,  2.30242275e-02, -4.64905234e-08,\n",
              "         1.62707530e-02, -9.96187925e-02,  7.74588883e-02, -9.34753641e-02,\n",
              "        -6.52854815e-02, -7.41244778e-02,  8.13705623e-02,  4.69529517e-02,\n",
              "         6.07407205e-02, -1.68937281e-01,  4.23962623e-03, -5.58095016e-02,\n",
              "        -5.58586046e-02,  2.89822854e-02,  1.25441998e-01,  2.62385197e-02,\n",
              "         9.10322666e-02,  3.83604281e-02,  1.56154903e-02, -1.46561032e-02,\n",
              "        -9.16609690e-02,  8.05214513e-03,  7.13425502e-02, -6.33465350e-02,\n",
              "        -4.47932296e-02,  6.31970763e-02,  2.25791205e-02,  9.99681875e-02,\n",
              "        -3.44539690e-03, -3.19939293e-02, -4.20683548e-02, -6.63160309e-02,\n",
              "         1.10528044e-01,  5.90667054e-02,  1.06190659e-01,  6.83258176e-02,\n",
              "         5.13570867e-02,  4.18496430e-02,  8.78782384e-03,  3.15269940e-02,\n",
              "        -6.91382214e-02, -1.62337944e-02,  4.83563617e-02,  3.13862711e-02,\n",
              "         6.51975349e-02, -8.46594647e-02,  8.09710398e-02,  7.08425418e-03,\n",
              "        -1.00689344e-01, -2.79401708e-02,  1.01260114e-02, -4.25480306e-02,\n",
              "         3.22192460e-02, -6.16578013e-02, -1.46406023e-02,  2.43320335e-02,\n",
              "        -7.99973756e-02, -6.08721711e-02,  2.63859462e-02,  6.39548376e-02,\n",
              "         6.83097690e-02, -4.24250476e-02, -5.69413882e-04,  5.20973504e-02],\n",
              "       dtype=float32),\n",
              " 'document_name': 'Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(documents_to_index))\n",
        "documents_to_index[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Ingest Documents into OpenSearch\n",
        "\n",
        "Now that we've processed the document and generated embeddings, let's ingest them into OpenSearch. We'll:\n",
        "\n",
        "1. Format each document with its text, embedding, and metadata\n",
        "2. Use the bulk API to efficiently insert all documents\n",
        "3. Verify that the documents were properly indexed\n",
        "\n",
        "This creates searchable content in the OpenSearch index that we can later query using hybrid search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "## each chunk is a document with its own embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Citation: Kim, Y.J.; Song, J.H.; Cho, K.H.; Shin, J.H.; Kim, J.S.; Yoon, J.S.; Hong, S.J. Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learning. Electronics 2024 ,13, 3577. https:// doi.org/10.3390/electronics13173577 Academic Editors: Claudio Turchetti and Laura Falaschetti Received: 31 July 2024 Revised: 2 September 2024 Accepted: 5 September 2024 Published: 9 September 2024 Copyright: ¬©2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). electronics Article Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learning Ye Jin Kim1, Jung Ho Song2, Ki Hwan Cho2, Jong Hyeon Shin2, Jong Sik Kim2 , Jung Sik Yoon2 and Sang Jeen Hong1,* 1Department of Semiconductor Engineering, Myongji University, Yongin 17058, Republic of Korea; 80231133@mju.ac.kr 2Plasma E.I. Convergence Research Center, Korea Research Institute of Fusion Energy, Daejeon 34133, Republic of Korea; jungho@kfe.re.kr (J.H.S.); khcho@kfe.re.kr (K.H.C.); shin0020@kfe.re.kr (J.H.S.); jongsik@kfe.re.kr (J.S.K.); jsyoon@kfe.re.kr (J.S.Y.) *Correspondence: samhong@mju.ac.kr Abstract: Existing etch endpoint detection (EPD) methods, primarily based on single wavelengths, have limitations, such as low signal-to-noise ratios and the inability to consider the long-term dependencies of time series data. To address these issues, this study proposes a context of time series data using long short-term memory (LSTM), a kind of recurrent neural network (RNN). The proposed method is based on the time series data collected through optical emission spectroscopy (OES) data during the SiO 2etching process. After training the LSTM model, the proposed method demonstrated the ability to detect the etch endpoint more accurately than existing methods by considering the entire time series. The LSTM model achieved an accuracy of 97.1% in a given condition, which shows that considering the flow and context of time series data can significantly reduce the false detection rate. To improve the performance of the proposed LSTM model, we created an attention-based LSTM model and confirmed that the model accuracy is 98.2%, and the performance is improved compared to that of the existing LSTM model. Keywords: plasma etch; endpoint detection; machine learning 1. Introduction Over the past decade, 3D-NAND flash memory technology has rapidly enhanced the bit storage density per unit area by increasing the number of vertically stacked gate layers. In recent years, the semiconductor industry has witnessed a significant increase in demand for high-capacity storage devices, such as 3D-NAND flash memory, driven by the rapid growth of various sectors, including cloud services and mobile devices. The latest generation of 3D-NAND features more than 200 layers of vertical gate stacks. To manufacture such a highly integrated 3D-NAND flash memory, high-aspect-ratio (HAR) etching technology is essential, enabling the precise etching of numerous layers, ranging in the several hundreds [ 1,2]. In these structures, accurately controlling the etching depth and precisely detecting the etching endpoint for each layer has become increasingly important. Consequently, reliable etch endpoint detection (EPD) techniques are crucial. EPD plays a vital role in determining the yield and quality of the etching process, and its'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents_to_index[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Index the documents in OpenSearch - so we are just adding index to documents -> create an action for each document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare bulk actions for OpenSearch\n",
        "actions = []\n",
        "for doc in documents_to_index:\n",
        "    # Handle asymmetric embedding if enabled\n",
        "    if ASSYMETRIC_EMBEDDING:\n",
        "        prefixed_text = f\"passage: {doc['text']}\"\n",
        "    else:\n",
        "        prefixed_text = doc['text']\n",
        "    \n",
        "    \n",
        "    # Create an action for this document\n",
        "    #you‚Äôre describing a series of actions for OpenSearch to perform.\n",
        "    action = {\n",
        "        \"_index\": OPENSEARCH_INDEX,\n",
        "        \"_id\": doc[\"doc_id\"],\n",
        "        \"_source\": {\n",
        "            \"text\": prefixed_text,\n",
        "            \"embedding\": doc[\"embedding\"].tolist(),  # Convert numpy array to list\n",
        "            \"document_name\": doc[\"document_name\"],\n",
        "        },\n",
        "    }\n",
        "    actions.append(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexing 15 documents into OpenSearch...\n",
            "Successfully indexed 15 documents\n"
          ]
        }
      ],
      "source": [
        "# Perform bulk indexing\n",
        "print(f\"Indexing {len(actions)} documents into OpenSearch...\")\n",
        "try:\n",
        "    success, errors = helpers.bulk(client, actions, raise_on_error=True)\n",
        "    if errors:\n",
        "        print(f\"Indexed {success} documents with {len(errors)} errors\")\n",
        "        print(f\"First error: {errors[0]}\")\n",
        "    else:\n",
        "        print(f\"Successfully indexed {success} documents\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during bulk indexing: {e}\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents in index: 0\n"
          ]
        }
      ],
      "source": [
        "  # Verify the documents are indexed\n",
        "response = client.count(index=OPENSEARCH_INDEX)\n",
        "print(f\"Total documents in index: {response['count']}\")\n",
        "\n",
        "# Get one document to verify content\n",
        "if response['count'] > 0:\n",
        "    sample = client.search(\n",
        "        index=OPENSEARCH_INDEX, \n",
        "        body={\n",
        "            \"size\": 1,\n",
        "            \"_source\": {\"excludes\": [\"embedding\"]},  # Exclude embeddings as they're large\n",
        "            \"query\": {\"match_all\": {}}\n",
        "        }\n",
        "    )\n",
        "    print(\"\\nSample document from index:\")\n",
        "    print(json.dumps(sample['hits']['hits'][0]['_source'], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search with keyword matching\n",
        "\n",
        "query = {\n",
        "    \"size\": 2,\n",
        "    \"_source\": {\"excludes\": [\"embedding\"]},\n",
        "    \"query\": {\n",
        "        \"match\": {\n",
        "            \"text\": \"tranformers\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "results = client.search(index=OPENSEARCH_INDEX, body=query)\n",
        "for hit in results['hits']['hits']:\n",
        "    print(json.dumps(hit['_source'], indent=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "\n",
            "Top 3 results for query: 'How can attention mechanism be used for CD predictions?'\n",
            "\n",
            "Result 1:\n",
            "{\n",
            "  \"document_name\": \"Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\",\n",
            "  \"text\": \"model. The acquired OES data are characterized by a gradual change in intensity as the etching process progresses, with different intensity widths at certain critical points where the intensity changes in unit time. To effectively capture these change features, we used an attention mechanism. The attention mechanism can focus on points in a sequence of OES data that have different intensity changes. It emphasizes the important parts of the sequence by assigning higher weights to points where the change in intensity represents a change from the previous state. During the learning process of the model, it evaluates the importance of each element of the input sequence and helps to ensure that important information is not lost [ 30]. The attention-based LSTM model was designed as shown in Figure 7. It consists of a basic LSTM layer that processes the input data, followed by an attention layer that evaluates the importance of each sequence. Finally, a fully connected layer is added to receive the output from the attention layer and generate the final predicted values. This ultimately produces an output that identifies the situations before and after the etch endpoint. The structure and parameter values of the attention-based LSTM model can be found in Table 7. Table 7. Attention-based LSTM model structure and parameter values. Layer (Type) Output Shape Parameter No. lstm (LSTM) (None, 10, 64) 18,688 lstm_1 (LSTM) (None, 10, 32) 12,416 Attention (Attention) (None, 32) 1088 dense (Dense) (None, 16) 528 dense_1 (Dense) (None, 1) 17 The training process for the attention-based LSTM model is the same as that of the LSTM model, with the only difference being the addition of the attention layer. When the training of the attention-based LSTM model was completed, it achieved an accuracy of approximately 98.2%. The performance of the model was evaluated using 10-fold crossvalidation and the loss and accuracy graphs of the model. The results showed that, similarElectronics 2024 ,13, 3577 9 of 11 to the LSTM model, the validation loss exhibited a similar trend, and no signs of overfitting were observed. Electronics \\u00a02024,\\u00a013,\\u00a0x\\u00a0FOR\\u00a0PEER\\u00a0REVIEW \\u00a0 9\\u00a0of\\u00a011\\u00a0 \\u00a0 \\u00a0to\\u00a0the\\u00a0LSTM\\u00a0model,\\u00a0the\\u00a0validation \\u00a0loss\\u00a0exhibited \\u00a0a\\u00a0similar\\u00a0trend,\\u00a0and\\u00a0no\\u00a0signs\\u00a0of\\u00a0over\\ufb01tting\\u00a0were\\u00a0observed. \\u00a0 \\u00a0 Figure\\u00a07.\\u00a0Attention-based \\u00a0LSTM\\u00a0neural\\u00a0network\\u00a0schematic. \\u00a0 As\\u00a0shown\\u00a0in\\u00a0Table\\u00a08,\\u00a0the\\u00a0attention-based \\u00a0LSTM\\u00a0model\\u00a0improved \\u00a0the\\u00a0etch\\u00a0endpoint \\u00a0 detection \\u00a0performance \\u00a0by\\u00a0approximately \\u00a01%\\u00a0and\\u00a0reduced\\u00a0the\\u00a0false\\u00a0detection \\u00a0rate\\u00a0(F1\\u00a0 score)\\u00a0by\\u00a0approximately \\u00a01%\\u00a0compared \\u00a0to\\u00a0the\\u00a0LSTM\\u00a0model.\\u00a0 Table\\u00a08.\\u00a0Attention-based \\u00a0LSTM\\u00a0model\\u00a0confusion \\u00a0matrix\\u00a0metrics.\\u00a0 \\u00a0 Precision \\u00a0 Recall\\u00a0 F1\\u00a0Score\\u00a0 Without\\u00a0EPD\\u00a0 0.98\\u00a0 0.98\\u00a0 0.98\\u00a0 With\\u00a0EPD\\u00a0 0.98\\u00a0 0.98\\u00a0 0.98\\u00a0 Accuracy \\u00a0\\u00a0\\u00a0 0.98\\u00a0 4.\\u00a0Conclusions \\u00a0 In\\u00a0this\\u00a0study,\\u00a0we\\u00a0proposed \\u00a0an\\u00a0LSTM-based \\u00a0method\\u00a0for\\u00a0detecting \\u00a0the\\u00a0endpoint \\u00a0in\\u00a0 etching\\u00a0processes. \\u00a0Unlike\\u00a0traditional \\u00a0approaches \\u00a0that\\u00a0rely\\u00a0on\\u00a0signal\\u00a0patterns\\u00a0at\\u00a0speci\\ufb01c\\u00a0 time\\u00a0points,\\u00a0this\\u00a0method\\u00a0demonstrated \\u00a0its\\u00a0e\\ufb00ectiveness \\u00a0by\\u00a0considering \\u00a0all\\u00a0the\\u00a0information \\u00a0 from\\u00a0the\\u00a0beginning \\u00a0to\\u00a0the\\u00a0end\\u00a0of\\u00a0the\\u00a0etching\\u00a0process\\u00a0contained \\u00a0in\\u00a0the\\u00a0entire\\u00a0OES\\u00a0time\\u00a0 series\\u00a0data,\\u00a0and\\u00a0the\\u00a0LSTM\\u00a0model\\u00a0achieved \\u00a0a\\u00a0high\\u00a0accuracy \\u00a0of\\u00a097.1%.\\u00a0The\\u00a0performance \\u00a0of\\u00a0 the\\u00a0model\\u00a0was\\u00a0further\\u00a0improved \\u00a0by\\u00a0applying \\u00a0the\\u00a0attention\\u00a0mechanism \\u00a0to\\u00a0the\\u00a0LSTM\\u00a0 model.\\u00a0The\\u00a0attention\\u00a0mechanism \\u00a0assigns\\u00a0higher\\u00a0weights\\u00a0to\\u00a0points\\u00a0in\\u00a0the\\u00a0time\\u00a0series\\u00a0data\\u00a0 where\\u00a0the\\u00a0intensity \\u00a0change\\u00a0di\\ufb00ers\\u00a0from\\u00a0the\\u00a0previous \\u00a0state,\\u00a0allowing \\u00a0the\\u00a0model\\u00a0to\\u00a0focus\\u00a0on\\u00a0 the\\u00a0critical\\u00a0information \\u00a0for\\u00a0etch\\u00a0endpoint \\u00a0detection. \\u00a0This\\u00a0resulted\\u00a0in\\u00a0an\\u00a0approximately \\u00a01%\\u00a0 performance \\u00a0improvement \\u00a0compared \\u00a0to\\u00a0the\\u00a0LSTM\\u00a0model,\\u00a0with\\u00a0the\\u00a0attention-based \\u00a0LSTM\\u00a0 model\\u00a0ultimately \\u00a0achieving \\u00a0a\\u00a0high\\u00a0accuracy \\u00a0of\\u00a098.2%.\\u00a0The\\u00a0proposed \\u00a0attention-based \\u00a0LSTM\\u00a0 model\\u00a0overcomes \\u00a0the\\u00a0limitations \\u00a0of\\u00a0the\\u00a0existing\\u00a0EPD\\u00a0algorithms \\u00a0and\\u00a0addresses \\u00a0the\\u00a0issues\\u00a0 of\\u00a0traditional \\u00a0methods \\u00a0that\\u00a0primarily \\u00a0focus\\u00a0on\\u00a0signal\\u00a0changes\\u00a0only\\u00a0at\\u00a0the\\u00a0precise\\u00a0EPD\\u00a0moment.\\u00a0By\\u00a0utilizing\\u00a0the\\u00a0temporal \\u00a0dependencies \\u00a0inherent\\u00a0in\\u00a0the\\u00a0entire\\u00a0OES\\u00a0time\\u00a0series\\u00a0data,\\u00a0 the\\u00a0proposed \\u00a0model\\u00a0not\\u00a0only\\u00a0accurately \\u00a0identi \\ufb01es\\u00a0the\\u00a0EPD\\u00a0moment\\u00a0but\\u00a0also\\u00a0captures \\u00a0the\\u00a0 precursory \\u00a0symptoms \\u00a0of\\u00a0EPD\\u00a0occurrence, \\u00a0enhancing \\u00a0the\\u00a0reliability \\u00a0of\\u00a0the\\u00a0system.\\u00a0In\\u00a0addition\\u00a0to\\u00a0improving \\u00a0endpoint \\u00a0detection \\u00a0accuracy, \\u00a0the\\u00a0proposed \\u00a0method\\u00a0also\\u00a0reduced\\u00a0the\\u00a0 false\\u00a0detection \\u00a0rate,\\u00a0which\\u00a0can\\u00a0negatively \\u00a0impact\\u00a0yield.\\u00a0This\\u00a0improvement \\u00a0can\\u00a0contribute \\u00a0 to\\u00a0enhanced \\u00a0EPD\\u00a0accuracy \\u00a0in\\u00a0high-aspect-ratio \\u00a0etching\\u00a0processes. \\u00a0Furthermore, \\u00a0the\\u00a0proposed\\u00a0LSTM-based \\u00a0method\\u00a0can\\u00a0be\\u00a0applied\\u00a0not\\u00a0only\\u00a0to\\u00a0the\\u00a0etching\\u00a0process\\u00a0but\\u00a0also\\u00a0to\\u00a0various\\u00a0stages\\u00a0of\\u00a0the\\u00a0semiconductor \\u00a0manufacturing \\u00a0process,\\u00a0such\\u00a0as\\u00a0cleaning\\u00a0processes, \\u00a0process\\u00a0monitoring, \\u00a0and\\u00a0anomaly \\u00a0detection. \\u00a0This\\u00a0can\\u00a0be\\u00a0utilized\\u00a0in\\u00a0various\\u00a0\\ufb01elds\\u00a0where\\u00a0time\\u00a0 Figure 7. Attention-based LSTM neural network schematic. As shown in Table 8, the attention-based LSTM model improved the etch endpoint detection performance\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "Result 2:\n",
            "{\n",
            "  \"document_name\": \"Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\",\n",
            "  \"text\": \"from\\u00a0the\\u00a0beginning \\u00a0to\\u00a0the\\u00a0end\\u00a0of\\u00a0the\\u00a0etching\\u00a0process\\u00a0contained \\u00a0in\\u00a0the\\u00a0entire\\u00a0OES\\u00a0time\\u00a0 series\\u00a0data,\\u00a0and\\u00a0the\\u00a0LSTM\\u00a0model\\u00a0achieved \\u00a0a\\u00a0high\\u00a0accuracy \\u00a0of\\u00a097.1%.\\u00a0The\\u00a0performance \\u00a0of\\u00a0 the\\u00a0model\\u00a0was\\u00a0further\\u00a0improved \\u00a0by\\u00a0applying \\u00a0the\\u00a0attention\\u00a0mechanism \\u00a0to\\u00a0the\\u00a0LSTM\\u00a0 model.\\u00a0The\\u00a0attention\\u00a0mechanism \\u00a0assigns\\u00a0higher\\u00a0weights\\u00a0to\\u00a0points\\u00a0in\\u00a0the\\u00a0time\\u00a0series\\u00a0data\\u00a0 where\\u00a0the\\u00a0intensity \\u00a0change\\u00a0di\\ufb00ers\\u00a0from\\u00a0the\\u00a0previous \\u00a0state,\\u00a0allowing \\u00a0the\\u00a0model\\u00a0to\\u00a0focus\\u00a0on\\u00a0 the\\u00a0critical\\u00a0information \\u00a0for\\u00a0etch\\u00a0endpoint \\u00a0detection. \\u00a0This\\u00a0resulted\\u00a0in\\u00a0an\\u00a0approximately \\u00a01%\\u00a0 performance \\u00a0improvement \\u00a0compared \\u00a0to\\u00a0the\\u00a0LSTM\\u00a0model,\\u00a0with\\u00a0the\\u00a0attention-based \\u00a0LSTM\\u00a0 model\\u00a0ultimately \\u00a0achieving \\u00a0a\\u00a0high\\u00a0accuracy \\u00a0of\\u00a098.2%.\\u00a0The\\u00a0proposed \\u00a0attention-based \\u00a0LSTM\\u00a0 model\\u00a0overcomes \\u00a0the\\u00a0limitations \\u00a0of\\u00a0the\\u00a0existing\\u00a0EPD\\u00a0algorithms \\u00a0and\\u00a0addresses \\u00a0the\\u00a0issues\\u00a0 of\\u00a0traditional \\u00a0methods \\u00a0that\\u00a0primarily \\u00a0focus\\u00a0on\\u00a0signal\\u00a0changes\\u00a0only\\u00a0at\\u00a0the\\u00a0precise\\u00a0EPD\\u00a0moment.\\u00a0By\\u00a0utilizing\\u00a0the\\u00a0temporal \\u00a0dependencies \\u00a0inherent\\u00a0in\\u00a0the\\u00a0entire\\u00a0OES\\u00a0time\\u00a0series\\u00a0data,\\u00a0 the\\u00a0proposed \\u00a0model\\u00a0not\\u00a0only\\u00a0accurately \\u00a0identi \\ufb01es\\u00a0the\\u00a0EPD\\u00a0moment\\u00a0but\\u00a0also\\u00a0captures \\u00a0the\\u00a0 precursory \\u00a0symptoms \\u00a0of\\u00a0EPD\\u00a0occurrence, \\u00a0enhancing \\u00a0the\\u00a0reliability \\u00a0of\\u00a0the\\u00a0system.\\u00a0In\\u00a0addition\\u00a0to\\u00a0improving \\u00a0endpoint \\u00a0detection \\u00a0accuracy, \\u00a0the\\u00a0proposed \\u00a0method\\u00a0also\\u00a0reduced\\u00a0the\\u00a0 false\\u00a0detection \\u00a0rate,\\u00a0which\\u00a0can\\u00a0negatively \\u00a0impact\\u00a0yield.\\u00a0This\\u00a0improvement \\u00a0can\\u00a0contribute \\u00a0 to\\u00a0enhanced \\u00a0EPD\\u00a0accuracy \\u00a0in\\u00a0high-aspect-ratio \\u00a0etching\\u00a0processes. \\u00a0Furthermore, \\u00a0the\\u00a0proposed\\u00a0LSTM-based \\u00a0method\\u00a0can\\u00a0be\\u00a0applied\\u00a0not\\u00a0only\\u00a0to\\u00a0the\\u00a0etching\\u00a0process\\u00a0but\\u00a0also\\u00a0to\\u00a0various\\u00a0stages\\u00a0of\\u00a0the\\u00a0semiconductor \\u00a0manufacturing \\u00a0process,\\u00a0such\\u00a0as\\u00a0cleaning\\u00a0processes, \\u00a0process\\u00a0monitoring, \\u00a0and\\u00a0anomaly \\u00a0detection. \\u00a0This\\u00a0can\\u00a0be\\u00a0utilized\\u00a0in\\u00a0various\\u00a0\\ufb01elds\\u00a0where\\u00a0time\\u00a0 Figure 7. Attention-based LSTM neural network schematic. As shown in Table 8, the attention-based LSTM model improved the etch endpoint detection performance by approximately 1% and reduced the false detection rate (F1 score) by approximately 1% compared to the LSTM model. Table 8. Attention-based LSTM model confusion matrix metrics. Precision Recall F1 Score Without EPD 0.98 0.98 0.98 With EPD 0.98 0.98 0.98 Accuracy 0.98 4. Conclusions In this study, we proposed an LSTM-based method for detecting the endpoint in etching processes. Unlike traditional approaches that rely on signal patterns at specific time points, this method demonstrated its effectiveness by considering all the information from the beginning to the end of the etching process contained in the entire OES time series data, and the LSTM model achieved a high accuracy of 97.1%. The performance of the model was further improved by applying the attention mechanism to the LSTM model. The attention mechanism assigns higher weights to points in the time series data where the intensity change differs from the previous state, allowing the model to focus on the critical information for etch endpoint detection. This resulted in an approximately 1% performance improvement compared to the LSTM model, with the attention-based LSTM model ultimately achieving a high accuracy of 98.2%. The proposed attention-based LSTM model overcomes the limitations of the existing EPD algorithms and addresses the issues of traditional methods that primarily focus on signal changes only at the precise EPD moment. By utilizing the temporal dependencies inherent in the entire OES time series data, the proposed model not only accurately identifies the EPD moment but also captures the precursory symptoms of EPD occurrence, enhancing the reliability of the system. In addition to improving endpoint detection accuracy, the proposed method also reduced the false detection rate, which can negatively impact yield. This improvement can contribute to enhanced EPD accuracy in high-aspect-ratio etching processes. Furthermore, the proposed LSTM-based method can be applied not only to the etching process but also to various stages of the semiconductor manufacturing process, such as cleaning processes, process monitoring, and anomaly detection. This can be utilized in various fields where timeElectronics 2024 ,13, 3577 10 of 11 series data analysis plays a crucial role and is expected to contribute to the technological advancement of semiconductor manufacturing processes. Author Contributions: Conceptualization, S.J.H., Y.J.K. and J.H.S. (Jong Hyeon Shin); methodology, S.J.H., Y.J.K. and K.H.C.; software, Y.J.K. and J.H.S. (Jong Hyeon Shin); validation, J.S.K. and J.H.S. (Jung Ho Song); formal analysis, Y.J.K. and S.J.H.; investigation, Y.J.K., J.H.S. (Jong Hyeon Shin) and S.J.H.;\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "Result 3:\n",
            "{\n",
            "  \"document_name\": \"Improved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory MachiImproved Plasma Etch Endpoint Detection Using Attention-Based Long Short-Term Memory Machine Learningne Learning\",\n",
            "  \"text\": \"and C, and the peaks related to the reaction products in the reaction surface mechanism, CO, CN, and SiF. The wavelengths used can be found in Table 2. Table 2. Information about the wavelengths used for EPD. Species Wavelength (nm) SiF 336, 440 CO 482, 560, 561 CN 386, 387 F 685, 677, 703 C2 516 O 777, 844Electronics 2024 ,13, 3577 4 of 11 All coupon wafers were etched for a pre-determined fixed time. To ensure the complete etching of the SiO 2layer, over-etching was performed. During this process, it was confirmed through ellipsometry that approximately 200 nm of the Si 3N4layer was etched. This was carried out to verify that the SiO 2layer was fully etched. Therefore, the optical emission signals collected through OES serve as time series data spanning the entire etching process. By using these data as input for the LSTM model, the model can learn long-term dependencies and detect the etch endpoint. 3. LSTM Method The LSTM is a model designed to address the limitations of the RNN, specifically the problems of vanishing gradients and exploding gradients. The gradient information in the LSTM is more effective in maintaining and utilizing information over longer sequences compared to the RNN [27]. As shown in Figure 2, the LSTM consists of three gates: the input gate, forget gate, and output gate. Each gate has the following roles. The input gate determines how to update the internal state based on the current state and the previous hidden state [ 28]. It decides what information should be stored in the cell state. First, it determines what information to store from the current input and the previous hidden state through a sigmoid layer. Then, the hyperbolic tangent (tanh) layer generates a new candidate vector. The forget gate determines what information should be forgotten and to what extent, from the previous cell state, by passing it through a sigmoid layer. The output gate determines how much of the cell state should be reflected in the hidden state and decides the hidden state to be passed to the next time step of the LSTM [28]. Electronics \\u00a02024,\\u00a013,\\u00a0x\\u00a0FOR\\u00a0PEER\\u00a0REVIEW \\u00a0 4\\u00a0of\\u00a011\\u00a0 \\u00a0 \\u00a0All\\u00a0coupon\\u00a0wafers\\u00a0were\\u00a0etched\\u00a0for\\u00a0a\\u00a0pre-determined \\u00a0\\ufb01xed\\u00a0time.\\u00a0To\\u00a0ensure\\u00a0the\\u00a0complete\\u00a0etching\\u00a0of\\u00a0the\\u00a0SiO 2\\u00a0layer,\\u00a0over-etching \\u00a0was\\u00a0performed. \\u00a0During\\u00a0this\\u00a0process,\\u00a0it\\u00a0was\\u00a0 con\\ufb01rmed\\u00a0through\\u00a0ellipsometry \\u00a0that\\u00a0approximately \\u00a0200\\u00a0nm\\u00a0of\\u00a0the\\u00a0Si3N4\\u00a0layer\\u00a0was\\u00a0etched.\\u00a0 This\\u00a0was\\u00a0carried\\u00a0out\\u00a0to\\u00a0verify\\u00a0that\\u00a0the\\u00a0SiO 2\\u00a0layer\\u00a0was\\u00a0fully\\u00a0etched.\\u00a0Therefore, \\u00a0the\\u00a0optical\\u00a0 emission \\u00a0signals\\u00a0collected \\u00a0through\\u00a0OES\\u00a0serve\\u00a0as\\u00a0time\\u00a0series\\u00a0data\\u00a0spanning \\u00a0the\\u00a0entire\\u00a0etching\\u00a0process.\\u00a0By\\u00a0using\\u00a0these\\u00a0data\\u00a0as\\u00a0input\\u00a0for\\u00a0the\\u00a0LSTM\\u00a0model,\\u00a0the\\u00a0model\\u00a0can\\u00a0learn\\u00a0longterm\\u00a0dependencies \\u00a0and\\u00a0detect\\u00a0the\\u00a0etch\\u00a0endpoint. \\u00a0 3.\\u00a0LSTM\\u00a0Method\\u00a0 The\\u00a0LSTM\\u00a0is\\u00a0a\\u00a0model\\u00a0designed \\u00a0to\\u00a0address\\u00a0the\\u00a0limitations \\u00a0of\\u00a0the\\u00a0RNN,\\u00a0speci\\ufb01cally\\u00a0the\\u00a0 problems \\u00a0of\\u00a0vanishing \\u00a0gradients \\u00a0and\\u00a0exploding \\u00a0gradients. \\u00a0The\\u00a0gradient\\u00a0information \\u00a0in\\u00a0the\\u00a0 LSTM\\u00a0is\\u00a0more\\u00a0e\\ufb00ective\\u00a0in\\u00a0maintaining \\u00a0and\\u00a0utilizing\\u00a0information \\u00a0over\\u00a0longer\\u00a0sequences \\u00a0 compared \\u00a0to\\u00a0the\\u00a0RNN\\u00a0[27].\\u00a0 As\\u00a0shown\\u00a0in\\u00a0Figure\\u00a02,\\u00a0the\\u00a0LSTM\\u00a0consists\\u00a0of\\u00a0three\\u00a0gates:\\u00a0the\\u00a0input\\u00a0gate,\\u00a0forget\\u00a0gate,\\u00a0 and\\u00a0output\\u00a0gate.\\u00a0Each\\u00a0gate\\u00a0has\\u00a0the\\u00a0following \\u00a0roles.\\u00a0The\\u00a0input\\u00a0gate\\u00a0determines \\u00a0how\\u00a0to\\u00a0update\\u00a0the\\u00a0internal\\u00a0state\\u00a0based\\u00a0on\\u00a0the\\u00a0current\\u00a0state\\u00a0and\\u00a0the\\u00a0previous \\u00a0hidden\\u00a0state\\u00a0[28].\\u00a0It\\u00a0 decides\\u00a0what\\u00a0information \\u00a0should\\u00a0be\\u00a0stored\\u00a0in\\u00a0the\\u00a0cell\\u00a0state.\\u00a0First,\\u00a0it\\u00a0determines \\u00a0what\\u00a0information \\u00a0to\\u00a0store\\u00a0from\\u00a0the\\u00a0current\\u00a0input\\u00a0and\\u00a0the\\u00a0previous \\u00a0hidden\\u00a0state\\u00a0through\\u00a0a\\u00a0sigmoid\\u00a0 layer.\\u00a0Then,\\u00a0the\\u00a0hyperbolic \\u00a0tangent\\u00a0(tanh)\\u00a0layer\\u00a0generates \\u00a0a\\u00a0new\\u00a0candidate \\u00a0vector.\\u00a0The\\u00a0 forget\\u00a0gate\\u00a0determines \\u00a0what\\u00a0information \\u00a0should\\u00a0be\\u00a0forgotten\\u00a0and\\u00a0to\\u00a0what\\u00a0extent,\\u00a0from\\u00a0the\\u00a0 previous \\u00a0cell\\u00a0state,\\u00a0by\\u00a0passing\\u00a0it\\u00a0through\\u00a0a\\u00a0sigmoid\\u00a0layer.\\u00a0The\\u00a0output\\u00a0gate\\u00a0determines \\u00a0how\\u00a0 much\\u00a0of\\u00a0the\\u00a0cell\\u00a0state\\u00a0should\\u00a0be\\u00a0re\\ufb02ected\\u00a0in\\u00a0the\\u00a0hidden\\u00a0state\\u00a0and\\u00a0decides\\u00a0the\\u00a0hidden\\u00a0state\\u00a0 to\\u00a0be\\u00a0passed\\u00a0to\\u00a0the\\u00a0next\\u00a0time\\u00a0step\\u00a0of\\u00a0the\\u00a0LSTM\\u00a0[28].\\u00a0 \\u00a0 Figure\\u00a02.\\u00a0LSTM\\u00a0diagram\\u00a0[29].\\u00a0 The\\u00a0LSTM\\u00a0controls\\u00a0the\\u00a0cell\\u00a0state\\u00a0using\\u00a0three\\u00a0gates.\\u00a0The\\u00a0cell\\u00a0state\\u00a0contains\\u00a0all\\u00a0the\\u00a0core\\u00a0 information, \\u00a0and\\u00a0the\\u00a0hidden\\u00a0state\\u00a0is\\u00a0processed \\u00a0whenever \\u00a0necessary \\u00a0to\\u00a0propagate \\u00a0information\\u00a0in\\u00a0a\\u00a0form\\u00a0that\\u00a0exposes\\u00a0only\\u00a0the\\u00a0required \\u00a0information \\u00a0for\\u00a0each\\u00a0time\\u00a0step.\\u00a0The\\u00a0equations\\u00a0for\\u00a0the\\u00a0LSTM\\u00a0are\\u00a0as\\u00a0follows:\\u00a0 \\ud835\\udc53\\u0be7\\u0d4c \\ud835\\udf0e\\u123a\\ud835\\udc4a\\u0bd9\\u2219\\u123e\\u210e\\u0be7\\u0b3f\\u0b35 ,\\ud835\\udc65\\u0be7\\u123f\\u0d45\\ud835\\udc4f\\u0bd9\\u123b,\\u00a0 (1) \\ud835\\udc56\\u0be7\\u0d4c \\ud835\\udf0e\\u0d6b\\ud835\\udc4a\\u0bdc\\u2219\\u0d63\\u210e\\u0be7\\u0b3f\\u0b35 ,\\ud835\\udc65\\u0be7\\u0d67\\u0d45\\ud835\\udc4f\\u0bdc\\u0d6f,\\u00a0\\u00a0 (2) \\ud835\\udc36\\u20d7\\u0be7\\u0d4c tanh\\u123a\\ud835\\udc4a\\u0bd6\\u2219\\u123e\\u210e\\u0be7\\u0b3f\\u0b35 ,\\ud835\\udc65\\u0be7\\u123f\\u0d45\\ud835\\udc4f\\u0bd6\\u123b,\\u00a0\\u00a0 (3) \\ud835\\udc36\\u0be7\\u0d4c \\ud835\\udc53\\u0be7\\u2219\\ud835\\udc36\\u0be7\\u0b3f\\u0b35\\u0d45\\ud835\\udc56\\u0be7\\u2219 \\ud835\\udc36\\u20d7 \\u0be7,\\u00a0 (4) \\ud835\\udc5c\\u0be7\\u0d4c \\ud835\\udf0e\\u123a\\ud835\\udc4a\\u0be2\\u2219\\u123e\\u210e\\u0be7\\u0b3f\\u0b35 ,\\ud835\\udc65\\u0be7\\u123f\\u0d45\\ud835\\udc4f\\u0be2\\u123b,\\u00a0 (5) \\u210e\\u0be7\\u0d4c \\ud835\\udc5c\\u0bdc\\u2219tanh \\u123a\\ud835\\udc36\\u0be7\\u123b\\u00a0 (6) Figure 2. LSTM diagram [29]. The LSTM controls the cell state using three gates. The cell state contains all the core information, and the hidden state is processed whenever necessary to propagate information in a form that exposes only the required information for each time step. The equations for the LSTM\"\n",
            "}\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#query_text = \"What are transformers?\"\n",
        "query_text = \"How can attention mechanism be used for CD predictions?\"\n",
        "\n",
        "# Generate embedding\n",
        "query_embedding = generate_embeddings([query_text])[0].tolist()\n",
        "\n",
        "# Set top_k\n",
        "top_k = 3\n",
        "\n",
        "query_body = {\n",
        "    \"_source\": {\"exclude\": [\"embedding\"]},\n",
        "    \"query\": {\n",
        "        \"hybrid\": {\n",
        "            \"queries\": [\n",
        "                {\"match\": {\"text\": {\"query\": query_text}}},\n",
        "                {\n",
        "                    \"knn\": {\n",
        "                        \"embedding\": {\n",
        "                            \"vector\": query_embedding,\n",
        "                            \"k\": top_k\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    \"size\": top_k\n",
        "}\n",
        "\n",
        "response = client.search(\n",
        "        index=OPENSEARCH_INDEX, body=query_body, search_pipeline=pipeline_name\n",
        "    )\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nTop {top_k} results for query: '{query_text}'\\n\")\n",
        "for i, hit in enumerate(response['hits']['hits'], 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(json.dumps(hit['_source'], indent=2))\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've successfully:\n",
        "1. Created an OpenSearch index with proper mappings for hybrid search\n",
        "2. Processed a PDF document and split it into chunks\n",
        "3. Generated embeddings for each chunk\n",
        "4. Ingested the chunks with their embeddings into OpenSearch\n",
        "\n",
        "All the code is defined directly in this notebook, so you can experiment with different:\n",
        "- Text cleaning and chunking strategies\n",
        "- Embedding models and parameters\n",
        "- OpenSearch index configurations\n",
        "\n",
        "In the next notebook, you'll learn how to perform hybrid search on this indexed content and generate responses using LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
